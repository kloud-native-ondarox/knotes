{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Kloud Native Knotes Brain Dump Site \u00b6 For years, I've caught most - if not all my notes in local repos with Markdown. Guess I finally decide to drop into public repo... This site is just a collection of Markdown files filled with random comments, code snippets, best practices I had captured while walking through demos, troubleshooting sessions, educational walkthroughs or from reading - do with it what you want.","title":"home"},{"location":"#welcome-to-kloud-native-knotes-brain-dump-site","text":"For years, I've caught most - if not all my notes in local repos with Markdown. Guess I finally decide to drop into public repo... This site is just a collection of Markdown files filled with random comments, code snippets, best practices I had captured while walking through demos, troubleshooting sessions, educational walkthroughs or from reading - do with it what you want.","title":"Welcome to Kloud Native Knotes Brain Dump Site"},{"location":"architecture-tldr/discovery-questions/","text":"How many of your org\u2019s applications are you planning to containerize? What types of applications are involved (Stateless? Stateful?) Where do you plan to deploy and run these applications (on-prem? On public cloud?) What are your biggest challenges with building and running cloud native workloads so far?","title":"Discovery questions"},{"location":"architecture-tldr/kubernetes-best-practices/references/","text":"Good References for Running Various Kubernetes Distributions in Production \u00b6 Capacity Planning \u00b6 Considerations for large clusters - https://kubernetes.io/docs/setup/best-practices/cluster-large/ Architecting Kubernetes clusters - choosing a worker node size - https://learnk8s.io/kubernetes-node-size Capacity Management \u00b6 Setting the right requests and limits in Kubernetes - https://learnk8s.io/setting-cpu-memory-limits-requests Production Ready - Best Practices \u00b6 Kubernetes Production Best Practices - https://learnk8s.io/production-best-practices Kubernetes Best Practices \u2013 Season One - https://medium.com/google-cloud/kubernetes-best-practices-season-one-11119aee1d1 Monitoring Application Metrics with Nutanix Karbon - https://medium.com/@christophe_99995/applications-metrics-monitoring-on-nutanix-karbon-c1d1158ebcfc Redhat Openshift Container Platform (OCP) References \u00b6 Disaster Recovery Strategies for Applications Running on OpenShift Spanning Multiple Sites(Data Centers/Regions) Considerations","title":"kubernetes best practices"},{"location":"architecture-tldr/kubernetes-best-practices/references/#good-references-for-running-various-kubernetes-distributions-in-production","text":"","title":"Good References for Running Various Kubernetes Distributions in Production"},{"location":"architecture-tldr/kubernetes-best-practices/references/#capacity-planning","text":"Considerations for large clusters - https://kubernetes.io/docs/setup/best-practices/cluster-large/ Architecting Kubernetes clusters - choosing a worker node size - https://learnk8s.io/kubernetes-node-size","title":"Capacity Planning"},{"location":"architecture-tldr/kubernetes-best-practices/references/#capacity-management","text":"Setting the right requests and limits in Kubernetes - https://learnk8s.io/setting-cpu-memory-limits-requests","title":"Capacity Management"},{"location":"architecture-tldr/kubernetes-best-practices/references/#production-ready-best-practices","text":"Kubernetes Production Best Practices - https://learnk8s.io/production-best-practices Kubernetes Best Practices \u2013 Season One - https://medium.com/google-cloud/kubernetes-best-practices-season-one-11119aee1d1 Monitoring Application Metrics with Nutanix Karbon - https://medium.com/@christophe_99995/applications-metrics-monitoring-on-nutanix-karbon-c1d1158ebcfc","title":"Production Ready - Best Practices"},{"location":"architecture-tldr/kubernetes-best-practices/references/#redhat-openshift-container-platform-ocp-references","text":"Disaster Recovery Strategies for Applications Running on OpenShift Spanning Multiple Sites(Data Centers/Regions) Considerations","title":"Redhat Openshift Container Platform (OCP) References"},{"location":"troubleshooting/kubernetes/etcd/","text":"etcd troubleshooting \u00b6 Health Checks of etcd members \u00b6 kubectl -n kube-system exec -it etcd-k8smaster-0 -- sh -c \"ETCDCTL_API=3 \\ ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt \\ ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt \\ ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key \\ etcdctl endpoint health\" Determine how many databases are part of the cluster. Three and five are common to provide 50up-arrow to return to the previous command and edit the command without having to type the whole command again. kubectl -n kube-system exec -it etcd-k8smaster-0 -- sh -c \"ETCDCTL_API=3 etcdctl --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --endpoints=https://127.0.0.1:2379 member list\" You can also view the status of the cluster in a table format. kubectl -n kube-system exec -it etcd-k8smaster-0 -- sh -c \"ETCDCTL_API=3 \\ ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt \\ ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt \\ ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key \\ etcdctl --endpoints=https://127.0.0.1:2379 \\ -w table endpoint status --cluster\" Snapshot and Restore ETCD DB \u00b6 Snapshot Etcd DB \u00b6 ETCDCTL_API=3 etcdctl snapshot save \"/tmp/etcd-backup.db\" --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key Write-out status \u00b6 ETCDCTL_API=3 etcdctl snapshot status \"/tmp/etcd-backup.db\" --write-out=table +----------+----------+------------+------------+ | HASH | REVISION | TOTAL KEYS | TOTAL SIZE | +----------+----------+------------+------------+ | 9012b5d5 | 23452 | 1479 | 2.5 MB | +----------+----------+------------+------------+ Restore Database \u00b6 ETCDCTL_API=3 etcdctl snapshot restore \"/tmp/etcd-backup.db\" --skip-hash-check=true --data-dir=\"/var/lib/etcd-from-backup\" Modify /etc/kubernetes/manifests/etcd.yaml \u00b6 hostPath: path: /var/lib/etcd-from-backup type: DirectoryOrCreate","title":"etcd"},{"location":"troubleshooting/kubernetes/etcd/#etcd-troubleshooting","text":"","title":"etcd troubleshooting"},{"location":"troubleshooting/kubernetes/etcd/#health-checks-of-etcd-members","text":"kubectl -n kube-system exec -it etcd-k8smaster-0 -- sh -c \"ETCDCTL_API=3 \\ ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt \\ ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt \\ ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key \\ etcdctl endpoint health\" Determine how many databases are part of the cluster. Three and five are common to provide 50up-arrow to return to the previous command and edit the command without having to type the whole command again. kubectl -n kube-system exec -it etcd-k8smaster-0 -- sh -c \"ETCDCTL_API=3 etcdctl --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --endpoints=https://127.0.0.1:2379 member list\" You can also view the status of the cluster in a table format. kubectl -n kube-system exec -it etcd-k8smaster-0 -- sh -c \"ETCDCTL_API=3 \\ ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt \\ ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt \\ ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key \\ etcdctl --endpoints=https://127.0.0.1:2379 \\ -w table endpoint status --cluster\"","title":"Health Checks of etcd members"},{"location":"troubleshooting/kubernetes/etcd/#snapshot-and-restore-etcd-db","text":"","title":"Snapshot and Restore ETCD DB"},{"location":"troubleshooting/kubernetes/etcd/#snapshot-etcd-db","text":"ETCDCTL_API=3 etcdctl snapshot save \"/tmp/etcd-backup.db\" --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key","title":"Snapshot Etcd DB"},{"location":"troubleshooting/kubernetes/etcd/#write-out-status","text":"ETCDCTL_API=3 etcdctl snapshot status \"/tmp/etcd-backup.db\" --write-out=table +----------+----------+------------+------------+ | HASH | REVISION | TOTAL KEYS | TOTAL SIZE | +----------+----------+------------+------------+ | 9012b5d5 | 23452 | 1479 | 2.5 MB | +----------+----------+------------+------------+","title":"Write-out status"},{"location":"troubleshooting/kubernetes/etcd/#restore-database","text":"ETCDCTL_API=3 etcdctl snapshot restore \"/tmp/etcd-backup.db\" --skip-hash-check=true --data-dir=\"/var/lib/etcd-from-backup\"","title":"Restore Database"},{"location":"troubleshooting/kubernetes/etcd/#modify-etckubernetesmanifestsetcdyaml","text":"hostPath: path: /var/lib/etcd-from-backup type: DirectoryOrCreate","title":"Modify /etc/kubernetes/manifests/etcd.yaml"},{"location":"troubleshooting/kubernetes/kubernetes/","text":"General Kubernetes Troubleshooting \u00b6 Connect to API server without KUBECTL Proxy \u00b6 Example Below: APISERVER=$(kubectl config view --minify | grep server | cut -f 2- -d \":\" | tr -d \" \") SECRET_NAME=$(kubectl get secrets | grep ^default | cut -f1 -d ' ') TOKEN=$(kubectl describe secret $SECRET_NAME | grep -E '^token' | cut -f2 -d':' | tr -d \" \") curl $APISERVER/api --header \"Authorization: Bearer $TOKEN\" --insecure alternative way of getting cluster info: APISERVER=$(kubectl config view --minify -ojsonpath='{.clusters[*].cluster.server}') Accessing the API from a Pod \u00b6 APISERVER=kubernetes.default.svc TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/ finding info using go-template: kubectl get pod redis-master-765d459796-258hz --template='{{(index (index .spec.containers 0).ports 0).containerPort}}{{\"\\n\"}}' vs. same info in jsonpath: kubectl get pod redis-master-6b54579d85-vkhdd -ojsonpath='{.spec.containers[0].ports[0].containerPort}{\"\\n\"}' Querying Stuff \u00b6 finding info using go-template: kubectl get pod redis-master-765d459796-258hz --template='{{(index (index .spec.containers 0).ports 0).containerPort}}{{\"\\n\"}}' vs. same info in jsonpath: kubectl get pod redis-master-6b54579d85-vkhdd -ojsonpath='{.spec.containers[0].ports[0].containerPort}{\"\\n\"}' using custom-columns kubectl get pod multi-cont-pod -o custom-columns=CONTAINER:.spec.containers[0].name,IMAGE:.spec.containers[0].image using sort-by kubectl get pods --sort-by=.metadata.name kubectl get pods --sort-by=.metadata.creationTimestamp using range with new line kubectl get po -l app=try -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}' Labels and Selectors \u00b6 overwrite label kubectl label --overwrite pods nginx2 app=v2 show labels kubectl get pods --show-labels get pods based on selector (equality based) kubectl get pods --selector=app=v2 kubectl get pods -l app=v2 kubectl get pods -l 'env in (dev,prod)' show label columns, i.e. app kubectl get pods --label-columns=app kubectl get pods -L app kubectl get pods -L app -L tier delete labels by appending - kubectl label pods nginx1 env- set labels on nodes kubectl label nodes kubernetes-foo-node-1.c.a-robinson.internal disktype=ssd port forwarding \u00b6 kubectl port-forward redis-master-765d459796-258hz 7000:6379 logging \u00b6 kubectl logs nginx --all-containers=true --prefix=true --since=60m --tail=20 --timestamps=true Events \u00b6 To monitor events in background kubectl get events -w & run fg and ctrl-c to kill process kubectl describe po busybox | grep -A 10 Events Resource Monitoring \u00b6 https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/ make sure metrics server is installed first either via helm - https://github.com/helm/charts/tree/master/stable/metrics-server or deployment components yaml - https://github.com/kubernetes-sigs/metrics-server Show metrics of the above pod containers and puts them into the file.log and verify kubectl -n kube-system get cm kubeadm-config -oyaml Events & Logs \u00b6 It can be easier if the data is actually sorted...sort by isn't for just events, it can be used in most output kubectl get events --sort-by='.metadata.creationTimestamp' Create a flawed deployment kubectl create deployment nginx --image ngins Time bounding your searches can be helpful in finding issues add --no-pager for line wrapping journalctl -u kubelet.service --since today --no-pager We can retrieve the logs for the control plane pods by using kubectl logs. This info is coming from the API server over kubectl, it instructs the kubelet will read the log from the node and send it back to you over stdout kubectl logs --namespace kube-system kube-apiserver-c1-master1 But, what if your control plane is down? Go to docker or to the file system. kubectl logs will send the request to the local node's kubelet to read the logs from disk. Since we're on the master/control plane node already we can use docker for that. sudo docker ps Grab the log for the api server pod, paste in the CONTAINER ID sudo docker ps | grep k8s_kube-apiserver CONTAINER_ID=$(sudo docker ps | grep k8s_kube-apiserver | awk '{ print $1 }') echo $CONTAINER_ID sudo docker logs $CONTAINER_ID But, what if docker is not available? They're also available on the filesystem, here you'll find the current and the previous logs files for the containers. This is the same across all nodes and pods in the cluster. This also applies to user pods/containers. These are json formmatted which is the docker logging driver default sudo ls /var/log/containers sudo tail /var/log/containers/kube-apiserver-c1-master1* We can filter the list of events using field selector kubectl get events --field-selector type=Warning kubectl get events --field-selector type=Warning,reason=Failed We're working with the json output of our objects, in this case pods let's start by accessing that list of Pods, inside items. Look at the items, find the metadata and name sections in the json output kubectl get pods -l app=hello-world -o json > pods.json It's a list of objects, so let's display the pod names kubectl get pods -l app=hello-world -o jsonpath='{ .items[*].metadata.name }' Display all pods names, this will put the new line at the end of the set rather then on each object output to screen. Additional tips on formatting code in the examples below including adding a new line after each object kubectl get pods -l app=hello-world -o jsonpath='{ .items[*].metadata.name }{\"\\n\"}' It's a list of objects, so let's display the first (zero'th) pod from the output kubectl get pods -l app=hello-world -o jsonpath='{ .items[0].metadata.name }{\"\\n\"}' Get all container images in use by all pods in all namespaces kubectl get pods --all-namespaces -o jsonpath='{ .items[*].spec.containers[*].image }{\"\\n\"}' We can access all container logs which will dump each containers in sequence kubectl logs $PODNAME --all-containers If we need to follow a log, we can do that...helpful in debugging real time issues. This works for both single and multi-container pods kubectl logs $PODNAME --all-containers --follow ctrl+c Get key information and status about the kubelet, ensure that it's active/running and check out the log. Also key information about it's configuration is available. systemctl status kubelet.service If we want to examine it's log further, we use journalctl to access it's log from journald -u for which systemd unit. If using a pager, use f and b to for forward and back. journalctl -u kubelet.service journalctl has search capabilities, but grep is likely easier journalctl -u kubelet.service | grep -i ERROR Time bounding your searches can be helpful in finding issues add --no-pager for line wrapping journalctl -u kubelet.service --since today --no-pager Get a listing of the control plane pods using a selector kubectl get pods --namespace kube-system --selector tier=control-plane Using Jsonpath \u00b6 This allows us to explore the json data interactively and keep our final jq query on the clipboard kubectl get no -o json | jid -q | pbcopy Filtering a specific value in a list Let's say there's an list inside items and you need to access an element in that list... ?() - defines a filter @ - the current object kubectl get nodes -o jsonpath=\"{.items[*].status.addresses''[?(@.type=='InternalIP')].address}\" Get all container images in use by all pods in all namespaces kubectl get pods --all-namespaces -o jsonpath='{ .items[*].spec.containers[*].image }{\"\\n\"}' Now that we're sorting that output, maybe we want a listing of all pods sorted by a field that's part of the object but not part of the default kubectl output. like creationTimestamp and we want to see what that value is We can use a custom colume to output object field data, in this case the creation timestamp kubectl get pods -A -o jsonpath='{ .items[*].metadata.name }{\"\\n\"}' \\ --sort-by=.metadata.creationTimestamp \\ --output=custom-columns='NAME:metadata.name,CREATIONTIMESTAMP:metadata.creationTimestamp' kubectl get po -A -o custom-columns=CREATE:.metadata.creationTimestamp,POD:.metadata.name,CONTAINER:.spec.containers[0].name,IMAGE:.spec.containers[0].image,PODIP:.status.podIP,HOSTIP:.status.hostIP,NS:.metadata.namespace One method to iterate through list $ kubectl get pods --all-namespaces -o jsonpath='{ .items[*].spec.containers[*].image }{\"\\n\"}' | tr \" \" \"\\n\" httpd:2.4-alpine httpd:2.4-alpine nginx:1.17.6-alpine nginx:1.17.6-alpine docker.io/calico/kube-controllers:v3.17.0 docker.io/calico/node:v3.17.0 quay.io/coreos/flannel:v0.12.0 docker.io/calico/node:v3.17.0 quay.io/coreos/flannel:v0.12.0 docker.io/calico/node:v3.17.0 quay.io/coreos/flannel:v0.12.0 k8s.gcr.io/coredns:1.7.0 k8s.gcr.io/coredns:1.7.0 k8s.gcr.io/etcd:3.4.13-0 k8s.gcr.io/kube-apiserver:v1.19.4 k8s.gcr.io/kube-controller-manager:v1.19.4 quay.io/coreos/flannel:v0.13.1-rc1 All container images across all pods in all namespaces. Range iterates over a list performing the formatting operations on each element in the list. We can also add in a sort on the container image name kubectl get pods -A -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.containers[*].image}{\"\\n\"}{end}' \\ --sort-by=.spec.containers[*].image We can use range again to clean up the output if we want kubectl get nodes -o jsonpath='{range .items[*]}{.status.addresses[?(@.type==\"InternalIP\")].address}{\"\\n\"}{end}' kubectl get nodes -o jsonpath='{range .items[*]}{.status.addresses[?(@.type==\"Hostname\")].address}{\"\\n\"}{end}' We used --sortby when looking at Events earlier, let's use it for another something else now... Let's take our container image output from above and sort it kubectl get pods -A -o jsonpath='{ .items[*].spec.containers[*].image }' --sort-by=.spec.containers[*].image kubectl get pods -A -o jsonpath='{range .items[*]}{.metadata.name }{\"\\t\"}{.spec.containers[*].image }{\"\\n\"}{end}' --sort-by=.spec.containers[*].image Adding in a spaces or tabs in the output to make it a bit more readable $ kubectl get pods -o jsonpath='{range .items[*]}{.metadata.name}{\" \"}{.spec.containers[*].image}{\"\\n\"}{end}' web-test-2-594487698d-jphg4 httpd:2.4-alpine web-test-6c77dcfbc-bqp4b httpd:2.4-alpine web-test-6c77dcfbc-vbnk7 httpd:2.4-alpine web-test-6c77dcfbc-wh4x4 httpd:2.4-alpine kubectl get pods -l app=hello-world -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.containers[*].image}{\"\\n\"}{end}' $ kubectl get pod -o jsonpath=\"{range .items[*]}{.metadata.name}{'\\n'}{end}\" web-test-2-594487698d-jphg4 web-test-6c77dcfbc-bqp4b web-test-6c77dcfbc-vbnk7 web-test-6c77dcfbc-wh4x4 Troubleshooting Node Failures \u00b6 Break Node \u00b6 To use this file to break stuff on your nodes, set the username variable to your username. This account will need sudo rights on the nodes to break things. You'll need to enter your sudo password for this account on each node for each execution. Execute the commands here one line at a time rather than running the whole script at ones. You can set up passwordless sudo to make this easier otherwise USER=$1 Worker Node - stopped kubelet `ssh $USER@c1-node1 -t 'sudo systemctl stop kubelet.service' `ssh $USER@c1-node1 -t 'sudo systemctl disable kubelet.service' Worker Node - inaccessible config.yaml `ssh $USER@c1-node2 -t 'sudo mv /var/lib/kubelet/config.yaml /var/lib/kubelet/config.yml' `ssh $USER@c1-node2 -t 'sudo systemctl restart kubelet.service' Worker Node - misconfigured systemd unit `ssh $USER@c1-node3 -t 'sudo sed -i ''s/config.yaml/config.yml/'' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf' `ssh $USER@c1-node3 -t 'sudo systemctl daemon-reload' `ssh $USER@c1-node3 -t 'sudo systemctl restart kubelet.service' The kubelet runs as a systemd service/unit...so we can use those tools to troubleshoot why it's not working Let's start by checking the status. Add no-pager so it will wrap the text - It's loaded, but it's inactive (dead)...so that means it's not running. We want the service to be active (running) So the first thing to check is the service enabled? sudo systemctl status kubelet.service If the service wasn't configured to start up by default (disabled) we can use enable to set it to. sudo systemctl enable kubelet.service That just enables the service to start up on boot, we could reboot now or we can start it manually So let's start it up and see what happens...ah, it's now actice (running) which means the kubelet is online. We also see in the journald snippet, that it's watching the apiserver. So good stuff there... sudo systemctl start kubelet.service sudo systemctl status kubelet.service Crashlooping kubelet...indicated by the code = exited and the status = 255 But that didn't tell us WHY the kubelet is crashlooping, just that it is...let's dig deeper sudo systemctl status kubelet.service --no-pager systemd based systems write logs to journald, let's ask it for the logs for the kubelet This tells us exactly what's wrong, the failed to load the Kubelet config file which it thinks is at /var/lib/kubelet/config.yaml sudo journalctl -u kubelet.service --no-pager Kubernetes Security \u00b6 Get Client Certificate Data from Kubectl Config kubectl config view --raw -o jsonpath=\"{.users[?(@.name=='k8s-admin')].user.client-certificate-data}\" | base64 -d Get Client Key from Kubectl Config $ kubectl config view --raw -o jsonpath=\"{.users[?(@.name=='k8s-admin')].user.client-key-data}\" | base64 -d -----BEGIN RSA PRIVATE KEY----- MIIEowIBAAKCAQEAqEs3hslI/1ndwsN8YG1GSP8DoBFfIJXjJW0+cGelYp6hs9lm gUmnsq9P0n26LNJpZI6SZ+lVTzgejisqF7mxXnf1kKTeRpoggo/nZ3WrLTpCjCuM JPUfFgKo178zmVfAILWipe3Ny/JuZF17oaiAIMmVK9OZMek2dvFSlSlVB2WVOp5P kVHFvJgQ5qYbsRW7F/l9x1dyQbVURCdRdTBMcyrOeU8lTPtH7Baceg2raxUJBbk8 SI4UJJlSPKKRkO6zmJ23PzLTFp3Cptrm17sUgw+aQ2UDUpnB7yIu4THlG3zh67W7 eTDteExV3TAjxNUbNKHPUzp2PB3wf53b2x0ONwIDAQABAoIBAATR6qw0lZ+inkRW vvgwCQRMMXljJftT76aBw3kKruTtMCprfpETX/cxKDMaILvp5tTXdH//Yc8cB1wB BnqZeef/vYu//RG+llHG91SyPQ3VjlRfZuskDhjeSKGtOzgYGEuXiCoCbpN5xQmg 18qgfdLykxAnRkr0p/euH7Rf86x7g2bktfmguZzdfBvISb8kIhk9o3uWc+1dTzw5 Y0nw5PzquGp0EWiTZsk9heK6gr/C0epq//g51aBpIoRGk3y7soUlF4Nkmnc9Nlqa d5Op3cL7ObdFSbgoNyY6JO8GNhqhLjVZ06LPQx968rC+AiudTCwrb8TizBp619tD SwgpSTECgYEAwDSmlh8mBVo2qR7A9XhfgvWdaCdAZZbi67bdktEETC77Xu6ukMj9 lhoHQz6oaJiB4O10fhiJDxK7zfks20WEjDnnaG7pEDsjb4ts5oGJ/tifCWUHZkZm ODEhDFJLwu6Bg70Bi7xhKeKs0so39s//1zemFzRAzcxQZCG6WkDsDckCgYEA4CbS Xz9W0TdExHz2zr0GPwn7bFZQIDIWHILWbU8tX+Zj5k/HMl1rTRQqePngFNKiNBCB hN+2wNepBGl/1k21RNnsXTRhgwyfEYecE2nFaqf8iobwxv6SSSuexwTay7o9rKtb iUHwpIXtxn3wRuvnbyjRLLBJlSsSGwOUXHFeO/8CgYB6ADGJcqYQma2+dZ3ncgu2 Na8/UELo+Ph6xC0qpu/CZ8P5AyndDycfos/fWCNPmRY/rpnV/D7rSWnaGQLm/95d n9eKC3R2cANTJz3tpmXwVJHGRdGHksIJgu3GQ2qBhiDBfTRA/UbzbkVi2ybgzDBJ 7LHJYsqLltekZ2BBL5pmOQKBgQCwH/D25EbsN1gyZ9pqEX6h8875jkyBL7nOB0RD OY52pwniAteLDHpuYyUIT5ax5duLu1h5tmrb1di5XcgT9JU1F2KwzaK9HSKz3HFX -k6mKJ5q4olT4lzkMg1jMGlVs9NbXIQHYtNZH//AYIga1Q1FjN5g8W/xFWEVusn5V sMKRswKBgEWWQ9peybZIaT4n9cGDoZBdp3cde6wYYae3n9zq2J9zUGuuCOlWlMHf 6ZekMDyUUS5OXhwmcMV5P8iJUq83rtGQfDhgTuECK1qMQYw+2eTgrZd3t+vk4X8c eMKcsY2p2nlSO3P7wdZfzGSDzWYl/mDFB3UfkNdT/mKWGv7xGftx -----END RSA PRIVATE KEY----- Read Certificate and Output in human readable format openssl x509 -in admin.crt -text -noout | head Accessing the API Server inside a Pod PODNAME=$(kubectl get pods -l app=nginx -o jsonpath='{ .items[*].metadata.name }') kubectl exec $PODNAME -it -- /bin/bash ls /var/run/secrets/kubernetes.io/serviceaccount/ cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt cat /var/run/secrets/kubernetes.io/serviceaccount/namespace cat /var/run/secrets/kubernetes.io/serviceaccount/token Load the token and cacert into variables for reuse TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt But it doesn't have any permissions to access objects...this user is not authorized to access pods `curl --cacert $CACERT --header \"Authorization: Bearer $TOKEN\" -X GET https://kubernetes.default.svc/api/v1/namespaces/ We can also use impersonation to help with our authorization testing kubectl auth can-i list pods --as=system:serviceaccount:default:mysvcaccount1 kubectl get pods -v 6 --as=system:serviceaccount:default:mysvcaccount1 But we can create an RBAC Role and bind that to our service account We define who, can perform what verbs on what resources kubectl create role demorole --verb=get,list --resource=pods kubectl create rolebinding demorolebinding --role=demorole --serviceaccount=default:mysvcaccount1 Then the service account can access the API with the https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions kubectl auth can-i list pods --as=system:serviceaccount:default:mysvcaccount1 kubectl get pods -v 6 --as=system:serviceaccount:default:mysvcaccount1 Go back inside the pod again... kubectl get pods PODNAME=$(kubectl get pods -l app=nginx -o jsonpath='{ .items[*].metadata.name }') kubectl exec $PODNAME -it -- /bin/bash Load the token and cacert into variables for reuse CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt Now I can view objects...this isn't just for curl but for any application. Apps commonly use libraries to programmaticly interact with the api server for cluster state information curl --cacert $CACERT --header \"Authorization: Bearer $TOKEN\" -X GET https://kubernetes.default.svc/api/v1/namespaces/ Investigating the PKI setup on the Control Plane Node \u00b6 The core pki directory, contains the certs and keys for all core functions in your cluster, the self signed CA, server certificate and key for encryption by API Server, etcd's cert setup, sa (serviceaccount) and more. ls -l /etc/kubernetes/pki Read the ca.crt to view the certificates information, useful to determine the validity date of the certificate You can use this command to read the information about any of the *.crt in this folder Be sure to check out the validity and the Subject CN openssl x509 -in /etc/kubernetes/pki/ca.crt -text -noout | more 2 - kubeconfig file location, for system components, controller manager, kubelet and scheduler. ls /etc/kubernetes certificate-authority-data is a base64 encoded ca.cert You can also see the server for the API Server is https And there is also a client-certificate-data which is the client certificate used. And client-key-data is the private key for the client cert. these are used to authenticate the client to the api server sudo more /etc/kubernetes/scheduler.conf The kube-proxy has it's kube-config as a configmap rather than a file on the file system. kubectl get configmap -n kube-system kube-proxy -o yaml Kubernetes - Create New User Certificate \u00b6 References: https://geekflare.com/openssl-commands-certificates/ https://kubernetes.io/docs/concepts/cluster-administration/certificates/#cfssl https://www.freecodecamp.org/news/openssl-command-cheatsheet-b441be1e8c4a/ https://www.digitalocean.com/community/tutorials/openssl-essentials-working-with-ssl-certificates-private-keys-and-csrs 1 - Create a RSA private key \u00b6 openssl genrsa -out demouser.key 2048 $ cat demouser.key -----BEGIN RSA PRIVATE KEY----- MIIEpAIBAAKCAQEAww0cP9PFhgXLSEMRTmvTKym2zcyMa9P5CgqGU9+MCv/Ngiw4 RjcuqsKyqIhetgzNVDHtlFA4zELuHbzgAv1uvky8DbINaf6aIgUnf2ZragpC8IQn lfHUIKapXdjqwbK3JQCY5W7ba0c3fdvOBHXkOij74tLDxtx/jP7x08zVE5UIiqeC XwOkiP1mjHHWsU8UkTKNnwjUFSoW/BUVQDJeK9WPtkd547djyOiFOgogkfXOy5F2 HH1n23qOFT8DaNL9KRfMgnQEIXVhBJclRHgGXun6ynFKE3WMZGKDpUq4wFaKvU2X mvEPIPR3Iu+pt0hJjSGpnlJWHfr6MQn7y4HO4QIDAQABAoIBAHtaxD3diXL8IRa/ S6ej63XFuNWogjoDYeGmzFMo8qFWK7siiihl576YyXJqZDOQHx8bQFxm67TKs1rd Q3LAopP5ZYjnzTH2kbXoOpWIyW/Ts4f2nC5pNTW9ESnH8Je1lbvyB8A5/sx2yrJv G3iYslDR8JL/pk8Szhv2dCv1w9/Qa2SlF2YCqy41V4Lih2n76cAZ7csC7PjynBVH h+m3Tz98gug6oEWfIMPpyTPLwCO2+P6f9hxtlFa8zWbXJ3MYiIn0DwMA0UEf40S4 qyEAU4c6jcFWdIEIRNTmJr2WwAfopP1v78plSMuqIo2rNns0o/ZvVkVer9AhEwR3 EjaAMVECgYEA4CpHHI1rjxLVI1LeOTCasOCI4csBeCSspJyqSppYAKy9F81iKBeV o9UfjvunZ6x8ZzZWzEFBv8YByTSKCd3Uuq/P33LQNgZjH2X4EZhP4a948CV+V0li +0h4kHLR7Te5ZuFePZ7ptoUf99Ao/N3JUATC2oD6VSaOQoJo3w0zat0CgYEA3sBe pLB4AGgKZPwOzHwVInJsbNC4R9w1ckZLFHZifZ89agbEvcGJW/jVgw8/E+SPNa3E 100WaDwa88864YCROIuF2KWtAa/D16nAf8hsk4uGO8RnqvpFB7qYlp5GSRSgTAMv /nbcjcCObEOAvK8ICBi4j/+GCVyRDBG2lYEGqdUCgYAfWgpkFetrMUkaDacC/KdG AcFjQw9LjGWRCFBQ6tFQFtjDkXge/11wcohdaRj6yQcFMHZnTuExPzJUv8Jmqt3r 1lcOe3Jfe/k1FP/jBhh2CiKyA6xt7NepKXOjUEvID7kgiHizyZwKaQgVksmIxEQ5 qtDN2qgobKIM70xXlfMRCQKBgQCf9tYAvxnucMjGLJ0UDCfBTRrAKkOsl19qaUCR uVKRlEGuWp3/B3V1LwVl0RUjXAfcLKYnV5y3zjIs1K0cNBAV41yDcLcFdwvVXHp5 SZ1vd8s2MJ2iE4hvPHlH8PHYmY9kBwX4X7OTuKyO4wsYdTn3Vol0H7RKFMe1OyM7 yiTW4QKBgQDeR4OfE4p61bJY+0rArEEtvXqReYzZAxqlGE0m6FL2zXNQ4MSqecgf 1lnzCUPcHfH2b0DsPAVDsJIaOmqLFVv7BXeG3J0OjGAwAIvDFHqMtp/36CYwWCN1 TahZVjHWx2+SqwxelhSZJHlRnGLqwfo8hqeb0CNnaiLcldX5WVnovw== -----END RSA PRIVATE KEY----- Verify private key file using openssl openssl rsa -in demouser.key -check Print public key using openssl openssl rsa -in demouser.key -pubout 2 - Generate a CSR \u00b6 CN (Common Name) is your username, O (Organization) is the Group If you get an error Can't load /home/USERNAME/.rnd into RNG - comment out RANDFILE from /etc/ssl/openssl.conf see this link for more details https://github.com/openssl/openssl/issues/7754#issuecomment-541307674 openssl req -new -key demouser.key -out demouser.csr -subj \"/CN=demouser\" or you can leave subject out and be prompted for additional information $ openssl req -new -key demouser.key -out demouser.csr You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name: 2-digit country code where your organization is legally located. State/Province: Write the full name of the state where your organization is legally located. City: Write the full name of the city where your organization is legally located. Organization Name: Write the legal name of your organization. Organization Unit: Name of the department (Not Compulsory. Press Enter to skip) Common Name: Your Fully Qualified Domain Name (e.g., www.yourdomainname.com.) Email: The email ID through which certification will take place (Not Compulsory. Press Enter to skip) The certificate request we'll use in the CertificateSigningRequest $ cat demouser.csr -----BEGIN CERTIFICATE REQUEST----- MIICWDCCAUACAQAwEzERMA8GA1UEAwwIZGVtb3VzZXIwggEiMA0GCSqGSIb3DQEB AQUAA4IBDwAwggEKAoIBAQDDDRw/08WGBctIQxFOa9MrKbbNzIxr0/kKCoZT34wK /82CLDhGNy6qwrKoiF62DM1UMe2UUDjMQu4dvOAC/W6+TLwNsg1p/poiBSd/Zmtq CkLwhCeV8dQgpqld2OrBsrclAJjlbttrRzd9284EdeQ6KPvi0sPG3H+M/vHTzNUT lQiKp4JfA6SI/WaMcdaxTxSRMo2fCNQVKhb8FRVAMl4r1Y+2R3njt2PI6IU6CiCR 9c7LkXYcfWfbeo4VPwNo0v0pF8yCdAQhdWEElyVEeAZe6frKcUoTdYxkYoOlSrjA Voq9TZea8Q8g9Hci76m3SEmNIameUlYd+voxCfvLgc7hAgMBAAGgADANBgkqhkiG 9w0BAQsFAAOCAQEARM18RbWm3225P61t9djzU21J0ftqSG2FPtYIL6hFSJFcwknq kG/DlUDiqAFBmDyS+iJCcEabouzbHewdrNEI+CstJu1n66FITCdkUmFdFnqnQBRB 6tvBSv0h/z0GioIRuLzgO1iWegl26a3TNt8I1S8YbJtTRnuV8GuVdKhm9BOYkMDZ dsS9uJ61zYN77HKVpiehyC94COzSMKGiipOzdu61BRDww/0X2rg1OVpy0z53ofUO HayIUOw7iYw2bueZpFpaP0vJ09lpwAu3KW5wUxT5Ng024oOfW6kT2dNa3epstqYQ IKy0TLJhJDqWkS942k6g82jYqz4+o0NruQ1HKw== -----END CERTIFICATE REQUEST----- Verify CSR file $ openssl req -in demouser.csr -noout -text -verify verify OK Certificate Request: Data: Version: 1 (0x0) Subject: C = US, ST = DE, L = Middletown, O = Home, OU = Lab, CN = demouser, emailAddress = no-reply@demo.user Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (2048 bit) The CertificateSigningRequest needs to be base64 encoded and also have the header and trailer pulled out. cat demouser.csr | base64 | tr -d \"\\n\" > demouser.base64.csr to decode: cat demouser.base64.csr | base64 -d ALTERNATIVE - Encode with Openssl Base64 cat demouser.csr | openssl enc -base64 -A > demouser.base64.csr to decode: cat demouser.base64.csr | openssl base64 -A -d 1B/2B - ALTERNATIVE - Create a new private key AND Certificate Signing Request \u00b6 The below command will generate CSR and a 2048-bit RSA key file. If you intend to use this certificate in Apache or Nginx, then you need to send this CSR file to certificate issuer authority, and they will give you a signed certificate mostly in der or pem format which you need to configure in Apache or Nginx web server. openssl req -out demouser.csr -newkey rsa:2048 -nodes -keyout demouser.key 3 - Submit the CertificateSigningRequest to the API Server - K8s 1.19+ \u00b6 UPDATE: If you're on 1.19+ use this CertificateSigningRequest Key elements, name, request and usages (must be client auth) cat <<EOF | kubectl apply -f - apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: demouser spec: groups: - system:authenticated request: $(cat demouser.base64.csr) signerName: kubernetes.io/kube-apiserver-client usages: - client auth EOF 3 - Submit the CertificateSigningRequest to the API Server - K8s 1.18 \u00b6 UPDATE: If you're on 1.18.x or below use this CertificateSigningRequest Key elements, name, request and usages (must be client auth) cat <<EOF | kubectl apply -f - apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: demouser spec: groups: - system:authenticated request: $(cat demouser.base64.csr) usages: - client auth EOF Let's get the CSR to see it's current state. The CSR will delete after an hour This should currently be Pending, awaiting administrative approval $ kubectl get certificatesigningrequests NAME AGE SIGNERNAME REQUESTOR CONDITION demouser 25s kubernetes.io/kube-apiserver-client k8s-admin Pending 4 - Approve the CSR \u00b6 kubectl certificate approve demouser If we get the state now, you'll see Approved, Issued. The CSR is updated with the certificate in .status.certificate $ kubectl get certificatesigningrequests demouser NAME AGE SIGNERNAME REQUESTOR CONDITION demouser 93s kubernetes.io/kube-apiserver-client k8s-admin Approved,Issued 5 - Retrieve the certificate from the CSR object, it's base64 encoded \u00b6 kubectl get certificatesigningrequests demouser \\ -o jsonpath='{ .status.certificate }' | base64 --decode Let's go ahead and save the certificate into a local file. We're going to use this file to build a kubeconfig file to authenticate to the API Server with kubectl get certificatesigningrequests demouser -o jsonpath='{ .status.certificate }' | base64 --decode > calmuser.crt Check the contents of the file cat demouser.crt Read the certficate itself Key elements: Issuer is our CA, Validity one year, Subject CN=demousers openssl x509 -in demouser.crt -text -noout | head -n 15 Now that we have the certificate we can use that to build a kubeconfig file with to log into this cluster. We'll use demouser.key and demouser.crt ls demouser.* COMPLETE ALTERNATIVE A - Create a Self-signed Certificate \u00b6 The below command will generate a self-signed certificate valid for two years with sha256 \u2013days parameter to extend the validity. Ex: to have self-signed valid for two years. openssl req -x509 -sha256 -nodes -days 730 -newkey rsa:2048 -keyout demouser_self.key -out demouser_cert.pem MISC ALTERNATIVE B \u00b6 Create a CSR from existing certificate and private key \u00b6 openssl x509 -x509toreq -in cert.pem -out example.csr -signkey example.key or just from existing private key openssl req \u2013out certificate.csr \u2013key existing.key \u2013new Generate a CSR for multi-domain SAN certificate by supplying an openssl config file: \u00b6 openssl req -new -key example.key -out example.csr -config req.conf cat <<EOF | tee req.conf [req]prompt=nodefault_md = sha256distinguished_name = dnreq_extensions = req_ext [dn]CN=example.com [req_ext]subjectAltName=@alt_names [alt_names]DNS.1=example.comDNS.2=www.example.comDNS.3=ftp.example.com EOF Create X.509 certificates \u00b6 Create self-signed certificate and new private key from scratch: openssl req -nodes -newkey rsa:2048 -keyout example.key -out example.crt -x509 -days 365 Create a self signed certificate using existing CSR and private key: openssl x509 -req -in example.csr -signkey example.key -out example.crt -days 365 Troubleshooting Certificate Issues \u00b6 Verify that private key matches a certificate, CSR and Private Key openssl verify demouser.crt openssl rsa -in demouser.key \u2013check openssl rsa -noout -modulus -in demouser.key | openssl sha256 openssl x509 -noout -modulus -in demouser.crt | openssl sha256 openssl req -noout -modulus -in demouser.csr | openssl sha256 Verify a Certificate was Signed by a CA openssl verify -verbose -CAFile ca.crt domain.crt Verifty the Certificate Signer Authority openssl x509 -in certfile.pem -text \u2013noout openssl x509 -in demouser_cert.pem -noout -issuer -issuer_hash Check Hash Value of A Certificate openssl x509 -noout -hash -in demouser_cert.pem Verify certificate, when you have intermediate certificate chain . Root certificate is not a part of bundle , and should be configured as a trusted on your machine. openssl verify -untrusted demouser-intermediate.pem demouser.crt Verify certificate, when you have intermediate certificate chain and root certificate , that is not configured as a trusted one. openssl verify -CAFile root.crt -untrusted intermediate-ca-chain.pem child-demouser.crt Verify that certificate served by a remote server covers given host name. Useful to check your mutlidomain certificate properly covers all the host names. openssl s_client -verify_hostname www.example.com -connect example.com:443 TLS client to connect to a remote server Test SSL certificate of particular URL Connect to a server supporting TLS: openssl s_client -connect example.com:443 openssl s_client -host example.com -port 443 openssl s_client -connect yoururl.com:443 \u2013showcerts Connect to a server and show full certificate chain: openssl s_client -showcerts -host example.com -port 443 </dev/null Extract the certificate: openssl s_client -connect example.com:443 2>&1 < /dev/null | sed -n '/-----BEGIN/,/-----END/p' > certificate.pem Override SNI (Server Name Indication) extension with another server name. Useful for testing when multiple secure sites are hosted on same IP address: openssl s_client -servername www.example.com -host example.com -port 443 Measure SSL connection time without/with session reuse: openssl s_time -connect example.com:443 -new openssl s_time -connect example.com:443 -reuse Convert between encoding and container formats Convert certificate between DER and PEM formats: openssl x509 -in example.pem -outform der -out example.der openssl x509 -in example.der -inform der -out example.pem Check PEM File Certificate Expiration Date openssl x509 -noout -in certificate.pem -dates Check Certificate Expiration Date of SSL URL openssl s_client -connect google.com:443 2>/dev/null | openssl x509 -noout -enddate Check TLS Versions are accepted on URL openssl s_client -connect secureurl.com:443 \u2013tls1 openssl s_client -connect secureurl.com:443 \u2013tls1_1 openssl s_client -showcerts -servername rancher.10.38.20.81.nip.io -connect rancher.10.38.20.81.nip.io:443 Capacity Planning \u00b6 Get vCPU Count from all nodes kubectl get nodes -o=jsonpath=\"{range .items[*]}{.metadata.name}{\\\"\\t\\\"} \\ {.status.capacity.cpu}{\\\"\\n\\\"}{end}\"","title":"kubernetes"},{"location":"troubleshooting/kubernetes/kubernetes/#general-kubernetes-troubleshooting","text":"","title":"General Kubernetes Troubleshooting"},{"location":"troubleshooting/kubernetes/kubernetes/#connect-to-api-server-without-kubectl-proxy","text":"Example Below: APISERVER=$(kubectl config view --minify | grep server | cut -f 2- -d \":\" | tr -d \" \") SECRET_NAME=$(kubectl get secrets | grep ^default | cut -f1 -d ' ') TOKEN=$(kubectl describe secret $SECRET_NAME | grep -E '^token' | cut -f2 -d':' | tr -d \" \") curl $APISERVER/api --header \"Authorization: Bearer $TOKEN\" --insecure alternative way of getting cluster info: APISERVER=$(kubectl config view --minify -ojsonpath='{.clusters[*].cluster.server}')","title":"Connect to API server without KUBECTL Proxy"},{"location":"troubleshooting/kubernetes/kubernetes/#accessing-the-api-from-a-pod","text":"APISERVER=kubernetes.default.svc TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/ finding info using go-template: kubectl get pod redis-master-765d459796-258hz --template='{{(index (index .spec.containers 0).ports 0).containerPort}}{{\"\\n\"}}' vs. same info in jsonpath: kubectl get pod redis-master-6b54579d85-vkhdd -ojsonpath='{.spec.containers[0].ports[0].containerPort}{\"\\n\"}'","title":"Accessing the API from a Pod"},{"location":"troubleshooting/kubernetes/kubernetes/#querying-stuff","text":"finding info using go-template: kubectl get pod redis-master-765d459796-258hz --template='{{(index (index .spec.containers 0).ports 0).containerPort}}{{\"\\n\"}}' vs. same info in jsonpath: kubectl get pod redis-master-6b54579d85-vkhdd -ojsonpath='{.spec.containers[0].ports[0].containerPort}{\"\\n\"}' using custom-columns kubectl get pod multi-cont-pod -o custom-columns=CONTAINER:.spec.containers[0].name,IMAGE:.spec.containers[0].image using sort-by kubectl get pods --sort-by=.metadata.name kubectl get pods --sort-by=.metadata.creationTimestamp using range with new line kubectl get po -l app=try -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}'","title":"Querying Stuff"},{"location":"troubleshooting/kubernetes/kubernetes/#labels-and-selectors","text":"overwrite label kubectl label --overwrite pods nginx2 app=v2 show labels kubectl get pods --show-labels get pods based on selector (equality based) kubectl get pods --selector=app=v2 kubectl get pods -l app=v2 kubectl get pods -l 'env in (dev,prod)' show label columns, i.e. app kubectl get pods --label-columns=app kubectl get pods -L app kubectl get pods -L app -L tier delete labels by appending - kubectl label pods nginx1 env- set labels on nodes kubectl label nodes kubernetes-foo-node-1.c.a-robinson.internal disktype=ssd","title":"Labels and Selectors"},{"location":"troubleshooting/kubernetes/kubernetes/#port-forwarding","text":"kubectl port-forward redis-master-765d459796-258hz 7000:6379","title":"port forwarding"},{"location":"troubleshooting/kubernetes/kubernetes/#logging","text":"kubectl logs nginx --all-containers=true --prefix=true --since=60m --tail=20 --timestamps=true","title":"logging"},{"location":"troubleshooting/kubernetes/kubernetes/#events","text":"To monitor events in background kubectl get events -w & run fg and ctrl-c to kill process kubectl describe po busybox | grep -A 10 Events","title":"Events"},{"location":"troubleshooting/kubernetes/kubernetes/#resource-monitoring","text":"https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/ make sure metrics server is installed first either via helm - https://github.com/helm/charts/tree/master/stable/metrics-server or deployment components yaml - https://github.com/kubernetes-sigs/metrics-server Show metrics of the above pod containers and puts them into the file.log and verify kubectl -n kube-system get cm kubeadm-config -oyaml","title":"Resource Monitoring"},{"location":"troubleshooting/kubernetes/kubernetes/#events-logs","text":"It can be easier if the data is actually sorted...sort by isn't for just events, it can be used in most output kubectl get events --sort-by='.metadata.creationTimestamp' Create a flawed deployment kubectl create deployment nginx --image ngins Time bounding your searches can be helpful in finding issues add --no-pager for line wrapping journalctl -u kubelet.service --since today --no-pager We can retrieve the logs for the control plane pods by using kubectl logs. This info is coming from the API server over kubectl, it instructs the kubelet will read the log from the node and send it back to you over stdout kubectl logs --namespace kube-system kube-apiserver-c1-master1 But, what if your control plane is down? Go to docker or to the file system. kubectl logs will send the request to the local node's kubelet to read the logs from disk. Since we're on the master/control plane node already we can use docker for that. sudo docker ps Grab the log for the api server pod, paste in the CONTAINER ID sudo docker ps | grep k8s_kube-apiserver CONTAINER_ID=$(sudo docker ps | grep k8s_kube-apiserver | awk '{ print $1 }') echo $CONTAINER_ID sudo docker logs $CONTAINER_ID But, what if docker is not available? They're also available on the filesystem, here you'll find the current and the previous logs files for the containers. This is the same across all nodes and pods in the cluster. This also applies to user pods/containers. These are json formmatted which is the docker logging driver default sudo ls /var/log/containers sudo tail /var/log/containers/kube-apiserver-c1-master1* We can filter the list of events using field selector kubectl get events --field-selector type=Warning kubectl get events --field-selector type=Warning,reason=Failed We're working with the json output of our objects, in this case pods let's start by accessing that list of Pods, inside items. Look at the items, find the metadata and name sections in the json output kubectl get pods -l app=hello-world -o json > pods.json It's a list of objects, so let's display the pod names kubectl get pods -l app=hello-world -o jsonpath='{ .items[*].metadata.name }' Display all pods names, this will put the new line at the end of the set rather then on each object output to screen. Additional tips on formatting code in the examples below including adding a new line after each object kubectl get pods -l app=hello-world -o jsonpath='{ .items[*].metadata.name }{\"\\n\"}' It's a list of objects, so let's display the first (zero'th) pod from the output kubectl get pods -l app=hello-world -o jsonpath='{ .items[0].metadata.name }{\"\\n\"}' Get all container images in use by all pods in all namespaces kubectl get pods --all-namespaces -o jsonpath='{ .items[*].spec.containers[*].image }{\"\\n\"}' We can access all container logs which will dump each containers in sequence kubectl logs $PODNAME --all-containers If we need to follow a log, we can do that...helpful in debugging real time issues. This works for both single and multi-container pods kubectl logs $PODNAME --all-containers --follow ctrl+c Get key information and status about the kubelet, ensure that it's active/running and check out the log. Also key information about it's configuration is available. systemctl status kubelet.service If we want to examine it's log further, we use journalctl to access it's log from journald -u for which systemd unit. If using a pager, use f and b to for forward and back. journalctl -u kubelet.service journalctl has search capabilities, but grep is likely easier journalctl -u kubelet.service | grep -i ERROR Time bounding your searches can be helpful in finding issues add --no-pager for line wrapping journalctl -u kubelet.service --since today --no-pager Get a listing of the control plane pods using a selector kubectl get pods --namespace kube-system --selector tier=control-plane","title":"Events &amp; Logs"},{"location":"troubleshooting/kubernetes/kubernetes/#using-jsonpath","text":"This allows us to explore the json data interactively and keep our final jq query on the clipboard kubectl get no -o json | jid -q | pbcopy Filtering a specific value in a list Let's say there's an list inside items and you need to access an element in that list... ?() - defines a filter @ - the current object kubectl get nodes -o jsonpath=\"{.items[*].status.addresses''[?(@.type=='InternalIP')].address}\" Get all container images in use by all pods in all namespaces kubectl get pods --all-namespaces -o jsonpath='{ .items[*].spec.containers[*].image }{\"\\n\"}' Now that we're sorting that output, maybe we want a listing of all pods sorted by a field that's part of the object but not part of the default kubectl output. like creationTimestamp and we want to see what that value is We can use a custom colume to output object field data, in this case the creation timestamp kubectl get pods -A -o jsonpath='{ .items[*].metadata.name }{\"\\n\"}' \\ --sort-by=.metadata.creationTimestamp \\ --output=custom-columns='NAME:metadata.name,CREATIONTIMESTAMP:metadata.creationTimestamp' kubectl get po -A -o custom-columns=CREATE:.metadata.creationTimestamp,POD:.metadata.name,CONTAINER:.spec.containers[0].name,IMAGE:.spec.containers[0].image,PODIP:.status.podIP,HOSTIP:.status.hostIP,NS:.metadata.namespace One method to iterate through list $ kubectl get pods --all-namespaces -o jsonpath='{ .items[*].spec.containers[*].image }{\"\\n\"}' | tr \" \" \"\\n\" httpd:2.4-alpine httpd:2.4-alpine nginx:1.17.6-alpine nginx:1.17.6-alpine docker.io/calico/kube-controllers:v3.17.0 docker.io/calico/node:v3.17.0 quay.io/coreos/flannel:v0.12.0 docker.io/calico/node:v3.17.0 quay.io/coreos/flannel:v0.12.0 docker.io/calico/node:v3.17.0 quay.io/coreos/flannel:v0.12.0 k8s.gcr.io/coredns:1.7.0 k8s.gcr.io/coredns:1.7.0 k8s.gcr.io/etcd:3.4.13-0 k8s.gcr.io/kube-apiserver:v1.19.4 k8s.gcr.io/kube-controller-manager:v1.19.4 quay.io/coreos/flannel:v0.13.1-rc1 All container images across all pods in all namespaces. Range iterates over a list performing the formatting operations on each element in the list. We can also add in a sort on the container image name kubectl get pods -A -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.containers[*].image}{\"\\n\"}{end}' \\ --sort-by=.spec.containers[*].image We can use range again to clean up the output if we want kubectl get nodes -o jsonpath='{range .items[*]}{.status.addresses[?(@.type==\"InternalIP\")].address}{\"\\n\"}{end}' kubectl get nodes -o jsonpath='{range .items[*]}{.status.addresses[?(@.type==\"Hostname\")].address}{\"\\n\"}{end}' We used --sortby when looking at Events earlier, let's use it for another something else now... Let's take our container image output from above and sort it kubectl get pods -A -o jsonpath='{ .items[*].spec.containers[*].image }' --sort-by=.spec.containers[*].image kubectl get pods -A -o jsonpath='{range .items[*]}{.metadata.name }{\"\\t\"}{.spec.containers[*].image }{\"\\n\"}{end}' --sort-by=.spec.containers[*].image Adding in a spaces or tabs in the output to make it a bit more readable $ kubectl get pods -o jsonpath='{range .items[*]}{.metadata.name}{\" \"}{.spec.containers[*].image}{\"\\n\"}{end}' web-test-2-594487698d-jphg4 httpd:2.4-alpine web-test-6c77dcfbc-bqp4b httpd:2.4-alpine web-test-6c77dcfbc-vbnk7 httpd:2.4-alpine web-test-6c77dcfbc-wh4x4 httpd:2.4-alpine kubectl get pods -l app=hello-world -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.containers[*].image}{\"\\n\"}{end}' $ kubectl get pod -o jsonpath=\"{range .items[*]}{.metadata.name}{'\\n'}{end}\" web-test-2-594487698d-jphg4 web-test-6c77dcfbc-bqp4b web-test-6c77dcfbc-vbnk7 web-test-6c77dcfbc-wh4x4","title":"Using Jsonpath"},{"location":"troubleshooting/kubernetes/kubernetes/#troubleshooting-node-failures","text":"","title":"Troubleshooting Node Failures"},{"location":"troubleshooting/kubernetes/kubernetes/#break-node","text":"To use this file to break stuff on your nodes, set the username variable to your username. This account will need sudo rights on the nodes to break things. You'll need to enter your sudo password for this account on each node for each execution. Execute the commands here one line at a time rather than running the whole script at ones. You can set up passwordless sudo to make this easier otherwise USER=$1 Worker Node - stopped kubelet `ssh $USER@c1-node1 -t 'sudo systemctl stop kubelet.service' `ssh $USER@c1-node1 -t 'sudo systemctl disable kubelet.service' Worker Node - inaccessible config.yaml `ssh $USER@c1-node2 -t 'sudo mv /var/lib/kubelet/config.yaml /var/lib/kubelet/config.yml' `ssh $USER@c1-node2 -t 'sudo systemctl restart kubelet.service' Worker Node - misconfigured systemd unit `ssh $USER@c1-node3 -t 'sudo sed -i ''s/config.yaml/config.yml/'' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf' `ssh $USER@c1-node3 -t 'sudo systemctl daemon-reload' `ssh $USER@c1-node3 -t 'sudo systemctl restart kubelet.service' The kubelet runs as a systemd service/unit...so we can use those tools to troubleshoot why it's not working Let's start by checking the status. Add no-pager so it will wrap the text - It's loaded, but it's inactive (dead)...so that means it's not running. We want the service to be active (running) So the first thing to check is the service enabled? sudo systemctl status kubelet.service If the service wasn't configured to start up by default (disabled) we can use enable to set it to. sudo systemctl enable kubelet.service That just enables the service to start up on boot, we could reboot now or we can start it manually So let's start it up and see what happens...ah, it's now actice (running) which means the kubelet is online. We also see in the journald snippet, that it's watching the apiserver. So good stuff there... sudo systemctl start kubelet.service sudo systemctl status kubelet.service Crashlooping kubelet...indicated by the code = exited and the status = 255 But that didn't tell us WHY the kubelet is crashlooping, just that it is...let's dig deeper sudo systemctl status kubelet.service --no-pager systemd based systems write logs to journald, let's ask it for the logs for the kubelet This tells us exactly what's wrong, the failed to load the Kubelet config file which it thinks is at /var/lib/kubelet/config.yaml sudo journalctl -u kubelet.service --no-pager","title":"Break Node"},{"location":"troubleshooting/kubernetes/kubernetes/#kubernetes-security","text":"Get Client Certificate Data from Kubectl Config kubectl config view --raw -o jsonpath=\"{.users[?(@.name=='k8s-admin')].user.client-certificate-data}\" | base64 -d Get Client Key from Kubectl Config $ kubectl config view --raw -o jsonpath=\"{.users[?(@.name=='k8s-admin')].user.client-key-data}\" | base64 -d -----BEGIN RSA PRIVATE KEY----- MIIEowIBAAKCAQEAqEs3hslI/1ndwsN8YG1GSP8DoBFfIJXjJW0+cGelYp6hs9lm gUmnsq9P0n26LNJpZI6SZ+lVTzgejisqF7mxXnf1kKTeRpoggo/nZ3WrLTpCjCuM JPUfFgKo178zmVfAILWipe3Ny/JuZF17oaiAIMmVK9OZMek2dvFSlSlVB2WVOp5P kVHFvJgQ5qYbsRW7F/l9x1dyQbVURCdRdTBMcyrOeU8lTPtH7Baceg2raxUJBbk8 SI4UJJlSPKKRkO6zmJ23PzLTFp3Cptrm17sUgw+aQ2UDUpnB7yIu4THlG3zh67W7 eTDteExV3TAjxNUbNKHPUzp2PB3wf53b2x0ONwIDAQABAoIBAATR6qw0lZ+inkRW vvgwCQRMMXljJftT76aBw3kKruTtMCprfpETX/cxKDMaILvp5tTXdH//Yc8cB1wB BnqZeef/vYu//RG+llHG91SyPQ3VjlRfZuskDhjeSKGtOzgYGEuXiCoCbpN5xQmg 18qgfdLykxAnRkr0p/euH7Rf86x7g2bktfmguZzdfBvISb8kIhk9o3uWc+1dTzw5 Y0nw5PzquGp0EWiTZsk9heK6gr/C0epq//g51aBpIoRGk3y7soUlF4Nkmnc9Nlqa d5Op3cL7ObdFSbgoNyY6JO8GNhqhLjVZ06LPQx968rC+AiudTCwrb8TizBp619tD SwgpSTECgYEAwDSmlh8mBVo2qR7A9XhfgvWdaCdAZZbi67bdktEETC77Xu6ukMj9 lhoHQz6oaJiB4O10fhiJDxK7zfks20WEjDnnaG7pEDsjb4ts5oGJ/tifCWUHZkZm ODEhDFJLwu6Bg70Bi7xhKeKs0so39s//1zemFzRAzcxQZCG6WkDsDckCgYEA4CbS Xz9W0TdExHz2zr0GPwn7bFZQIDIWHILWbU8tX+Zj5k/HMl1rTRQqePngFNKiNBCB hN+2wNepBGl/1k21RNnsXTRhgwyfEYecE2nFaqf8iobwxv6SSSuexwTay7o9rKtb iUHwpIXtxn3wRuvnbyjRLLBJlSsSGwOUXHFeO/8CgYB6ADGJcqYQma2+dZ3ncgu2 Na8/UELo+Ph6xC0qpu/CZ8P5AyndDycfos/fWCNPmRY/rpnV/D7rSWnaGQLm/95d n9eKC3R2cANTJz3tpmXwVJHGRdGHksIJgu3GQ2qBhiDBfTRA/UbzbkVi2ybgzDBJ 7LHJYsqLltekZ2BBL5pmOQKBgQCwH/D25EbsN1gyZ9pqEX6h8875jkyBL7nOB0RD OY52pwniAteLDHpuYyUIT5ax5duLu1h5tmrb1di5XcgT9JU1F2KwzaK9HSKz3HFX -k6mKJ5q4olT4lzkMg1jMGlVs9NbXIQHYtNZH//AYIga1Q1FjN5g8W/xFWEVusn5V sMKRswKBgEWWQ9peybZIaT4n9cGDoZBdp3cde6wYYae3n9zq2J9zUGuuCOlWlMHf 6ZekMDyUUS5OXhwmcMV5P8iJUq83rtGQfDhgTuECK1qMQYw+2eTgrZd3t+vk4X8c eMKcsY2p2nlSO3P7wdZfzGSDzWYl/mDFB3UfkNdT/mKWGv7xGftx -----END RSA PRIVATE KEY----- Read Certificate and Output in human readable format openssl x509 -in admin.crt -text -noout | head Accessing the API Server inside a Pod PODNAME=$(kubectl get pods -l app=nginx -o jsonpath='{ .items[*].metadata.name }') kubectl exec $PODNAME -it -- /bin/bash ls /var/run/secrets/kubernetes.io/serviceaccount/ cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt cat /var/run/secrets/kubernetes.io/serviceaccount/namespace cat /var/run/secrets/kubernetes.io/serviceaccount/token Load the token and cacert into variables for reuse TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt But it doesn't have any permissions to access objects...this user is not authorized to access pods `curl --cacert $CACERT --header \"Authorization: Bearer $TOKEN\" -X GET https://kubernetes.default.svc/api/v1/namespaces/ We can also use impersonation to help with our authorization testing kubectl auth can-i list pods --as=system:serviceaccount:default:mysvcaccount1 kubectl get pods -v 6 --as=system:serviceaccount:default:mysvcaccount1 But we can create an RBAC Role and bind that to our service account We define who, can perform what verbs on what resources kubectl create role demorole --verb=get,list --resource=pods kubectl create rolebinding demorolebinding --role=demorole --serviceaccount=default:mysvcaccount1 Then the service account can access the API with the https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions kubectl auth can-i list pods --as=system:serviceaccount:default:mysvcaccount1 kubectl get pods -v 6 --as=system:serviceaccount:default:mysvcaccount1 Go back inside the pod again... kubectl get pods PODNAME=$(kubectl get pods -l app=nginx -o jsonpath='{ .items[*].metadata.name }') kubectl exec $PODNAME -it -- /bin/bash Load the token and cacert into variables for reuse CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt Now I can view objects...this isn't just for curl but for any application. Apps commonly use libraries to programmaticly interact with the api server for cluster state information curl --cacert $CACERT --header \"Authorization: Bearer $TOKEN\" -X GET https://kubernetes.default.svc/api/v1/namespaces/","title":"Kubernetes Security"},{"location":"troubleshooting/kubernetes/kubernetes/#investigating-the-pki-setup-on-the-control-plane-node","text":"The core pki directory, contains the certs and keys for all core functions in your cluster, the self signed CA, server certificate and key for encryption by API Server, etcd's cert setup, sa (serviceaccount) and more. ls -l /etc/kubernetes/pki Read the ca.crt to view the certificates information, useful to determine the validity date of the certificate You can use this command to read the information about any of the *.crt in this folder Be sure to check out the validity and the Subject CN openssl x509 -in /etc/kubernetes/pki/ca.crt -text -noout | more 2 - kubeconfig file location, for system components, controller manager, kubelet and scheduler. ls /etc/kubernetes certificate-authority-data is a base64 encoded ca.cert You can also see the server for the API Server is https And there is also a client-certificate-data which is the client certificate used. And client-key-data is the private key for the client cert. these are used to authenticate the client to the api server sudo more /etc/kubernetes/scheduler.conf The kube-proxy has it's kube-config as a configmap rather than a file on the file system. kubectl get configmap -n kube-system kube-proxy -o yaml","title":"Investigating the PKI setup on the Control Plane Node"},{"location":"troubleshooting/kubernetes/kubernetes/#kubernetes-create-new-user-certificate","text":"References: https://geekflare.com/openssl-commands-certificates/ https://kubernetes.io/docs/concepts/cluster-administration/certificates/#cfssl https://www.freecodecamp.org/news/openssl-command-cheatsheet-b441be1e8c4a/ https://www.digitalocean.com/community/tutorials/openssl-essentials-working-with-ssl-certificates-private-keys-and-csrs","title":"Kubernetes - Create New User Certificate"},{"location":"troubleshooting/kubernetes/kubernetes/#1-create-a-rsa-private-key","text":"openssl genrsa -out demouser.key 2048 $ cat demouser.key -----BEGIN RSA PRIVATE KEY----- MIIEpAIBAAKCAQEAww0cP9PFhgXLSEMRTmvTKym2zcyMa9P5CgqGU9+MCv/Ngiw4 RjcuqsKyqIhetgzNVDHtlFA4zELuHbzgAv1uvky8DbINaf6aIgUnf2ZragpC8IQn lfHUIKapXdjqwbK3JQCY5W7ba0c3fdvOBHXkOij74tLDxtx/jP7x08zVE5UIiqeC XwOkiP1mjHHWsU8UkTKNnwjUFSoW/BUVQDJeK9WPtkd547djyOiFOgogkfXOy5F2 HH1n23qOFT8DaNL9KRfMgnQEIXVhBJclRHgGXun6ynFKE3WMZGKDpUq4wFaKvU2X mvEPIPR3Iu+pt0hJjSGpnlJWHfr6MQn7y4HO4QIDAQABAoIBAHtaxD3diXL8IRa/ S6ej63XFuNWogjoDYeGmzFMo8qFWK7siiihl576YyXJqZDOQHx8bQFxm67TKs1rd Q3LAopP5ZYjnzTH2kbXoOpWIyW/Ts4f2nC5pNTW9ESnH8Je1lbvyB8A5/sx2yrJv G3iYslDR8JL/pk8Szhv2dCv1w9/Qa2SlF2YCqy41V4Lih2n76cAZ7csC7PjynBVH h+m3Tz98gug6oEWfIMPpyTPLwCO2+P6f9hxtlFa8zWbXJ3MYiIn0DwMA0UEf40S4 qyEAU4c6jcFWdIEIRNTmJr2WwAfopP1v78plSMuqIo2rNns0o/ZvVkVer9AhEwR3 EjaAMVECgYEA4CpHHI1rjxLVI1LeOTCasOCI4csBeCSspJyqSppYAKy9F81iKBeV o9UfjvunZ6x8ZzZWzEFBv8YByTSKCd3Uuq/P33LQNgZjH2X4EZhP4a948CV+V0li +0h4kHLR7Te5ZuFePZ7ptoUf99Ao/N3JUATC2oD6VSaOQoJo3w0zat0CgYEA3sBe pLB4AGgKZPwOzHwVInJsbNC4R9w1ckZLFHZifZ89agbEvcGJW/jVgw8/E+SPNa3E 100WaDwa88864YCROIuF2KWtAa/D16nAf8hsk4uGO8RnqvpFB7qYlp5GSRSgTAMv /nbcjcCObEOAvK8ICBi4j/+GCVyRDBG2lYEGqdUCgYAfWgpkFetrMUkaDacC/KdG AcFjQw9LjGWRCFBQ6tFQFtjDkXge/11wcohdaRj6yQcFMHZnTuExPzJUv8Jmqt3r 1lcOe3Jfe/k1FP/jBhh2CiKyA6xt7NepKXOjUEvID7kgiHizyZwKaQgVksmIxEQ5 qtDN2qgobKIM70xXlfMRCQKBgQCf9tYAvxnucMjGLJ0UDCfBTRrAKkOsl19qaUCR uVKRlEGuWp3/B3V1LwVl0RUjXAfcLKYnV5y3zjIs1K0cNBAV41yDcLcFdwvVXHp5 SZ1vd8s2MJ2iE4hvPHlH8PHYmY9kBwX4X7OTuKyO4wsYdTn3Vol0H7RKFMe1OyM7 yiTW4QKBgQDeR4OfE4p61bJY+0rArEEtvXqReYzZAxqlGE0m6FL2zXNQ4MSqecgf 1lnzCUPcHfH2b0DsPAVDsJIaOmqLFVv7BXeG3J0OjGAwAIvDFHqMtp/36CYwWCN1 TahZVjHWx2+SqwxelhSZJHlRnGLqwfo8hqeb0CNnaiLcldX5WVnovw== -----END RSA PRIVATE KEY----- Verify private key file using openssl openssl rsa -in demouser.key -check Print public key using openssl openssl rsa -in demouser.key -pubout","title":"1 - Create a RSA private key"},{"location":"troubleshooting/kubernetes/kubernetes/#2-generate-a-csr","text":"CN (Common Name) is your username, O (Organization) is the Group If you get an error Can't load /home/USERNAME/.rnd into RNG - comment out RANDFILE from /etc/ssl/openssl.conf see this link for more details https://github.com/openssl/openssl/issues/7754#issuecomment-541307674 openssl req -new -key demouser.key -out demouser.csr -subj \"/CN=demouser\" or you can leave subject out and be prompted for additional information $ openssl req -new -key demouser.key -out demouser.csr You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name: 2-digit country code where your organization is legally located. State/Province: Write the full name of the state where your organization is legally located. City: Write the full name of the city where your organization is legally located. Organization Name: Write the legal name of your organization. Organization Unit: Name of the department (Not Compulsory. Press Enter to skip) Common Name: Your Fully Qualified Domain Name (e.g., www.yourdomainname.com.) Email: The email ID through which certification will take place (Not Compulsory. Press Enter to skip) The certificate request we'll use in the CertificateSigningRequest $ cat demouser.csr -----BEGIN CERTIFICATE REQUEST----- MIICWDCCAUACAQAwEzERMA8GA1UEAwwIZGVtb3VzZXIwggEiMA0GCSqGSIb3DQEB AQUAA4IBDwAwggEKAoIBAQDDDRw/08WGBctIQxFOa9MrKbbNzIxr0/kKCoZT34wK /82CLDhGNy6qwrKoiF62DM1UMe2UUDjMQu4dvOAC/W6+TLwNsg1p/poiBSd/Zmtq CkLwhCeV8dQgpqld2OrBsrclAJjlbttrRzd9284EdeQ6KPvi0sPG3H+M/vHTzNUT lQiKp4JfA6SI/WaMcdaxTxSRMo2fCNQVKhb8FRVAMl4r1Y+2R3njt2PI6IU6CiCR 9c7LkXYcfWfbeo4VPwNo0v0pF8yCdAQhdWEElyVEeAZe6frKcUoTdYxkYoOlSrjA Voq9TZea8Q8g9Hci76m3SEmNIameUlYd+voxCfvLgc7hAgMBAAGgADANBgkqhkiG 9w0BAQsFAAOCAQEARM18RbWm3225P61t9djzU21J0ftqSG2FPtYIL6hFSJFcwknq kG/DlUDiqAFBmDyS+iJCcEabouzbHewdrNEI+CstJu1n66FITCdkUmFdFnqnQBRB 6tvBSv0h/z0GioIRuLzgO1iWegl26a3TNt8I1S8YbJtTRnuV8GuVdKhm9BOYkMDZ dsS9uJ61zYN77HKVpiehyC94COzSMKGiipOzdu61BRDww/0X2rg1OVpy0z53ofUO HayIUOw7iYw2bueZpFpaP0vJ09lpwAu3KW5wUxT5Ng024oOfW6kT2dNa3epstqYQ IKy0TLJhJDqWkS942k6g82jYqz4+o0NruQ1HKw== -----END CERTIFICATE REQUEST----- Verify CSR file $ openssl req -in demouser.csr -noout -text -verify verify OK Certificate Request: Data: Version: 1 (0x0) Subject: C = US, ST = DE, L = Middletown, O = Home, OU = Lab, CN = demouser, emailAddress = no-reply@demo.user Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (2048 bit) The CertificateSigningRequest needs to be base64 encoded and also have the header and trailer pulled out. cat demouser.csr | base64 | tr -d \"\\n\" > demouser.base64.csr to decode: cat demouser.base64.csr | base64 -d ALTERNATIVE - Encode with Openssl Base64 cat demouser.csr | openssl enc -base64 -A > demouser.base64.csr to decode: cat demouser.base64.csr | openssl base64 -A -d","title":"2 - Generate a CSR"},{"location":"troubleshooting/kubernetes/kubernetes/#1b2b-alternative-create-a-new-private-key-and-certificate-signing-request","text":"The below command will generate CSR and a 2048-bit RSA key file. If you intend to use this certificate in Apache or Nginx, then you need to send this CSR file to certificate issuer authority, and they will give you a signed certificate mostly in der or pem format which you need to configure in Apache or Nginx web server. openssl req -out demouser.csr -newkey rsa:2048 -nodes -keyout demouser.key","title":"1B/2B - ALTERNATIVE - Create a new private key AND Certificate Signing Request"},{"location":"troubleshooting/kubernetes/kubernetes/#3-submit-the-certificatesigningrequest-to-the-api-server-k8s-119","text":"UPDATE: If you're on 1.19+ use this CertificateSigningRequest Key elements, name, request and usages (must be client auth) cat <<EOF | kubectl apply -f - apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: demouser spec: groups: - system:authenticated request: $(cat demouser.base64.csr) signerName: kubernetes.io/kube-apiserver-client usages: - client auth EOF","title":"3 - Submit the CertificateSigningRequest to the API Server - K8s 1.19+"},{"location":"troubleshooting/kubernetes/kubernetes/#3-submit-the-certificatesigningrequest-to-the-api-server-k8s-118","text":"UPDATE: If you're on 1.18.x or below use this CertificateSigningRequest Key elements, name, request and usages (must be client auth) cat <<EOF | kubectl apply -f - apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: demouser spec: groups: - system:authenticated request: $(cat demouser.base64.csr) usages: - client auth EOF Let's get the CSR to see it's current state. The CSR will delete after an hour This should currently be Pending, awaiting administrative approval $ kubectl get certificatesigningrequests NAME AGE SIGNERNAME REQUESTOR CONDITION demouser 25s kubernetes.io/kube-apiserver-client k8s-admin Pending","title":"3 - Submit the CertificateSigningRequest to the API Server - K8s 1.18"},{"location":"troubleshooting/kubernetes/kubernetes/#4-approve-the-csr","text":"kubectl certificate approve demouser If we get the state now, you'll see Approved, Issued. The CSR is updated with the certificate in .status.certificate $ kubectl get certificatesigningrequests demouser NAME AGE SIGNERNAME REQUESTOR CONDITION demouser 93s kubernetes.io/kube-apiserver-client k8s-admin Approved,Issued","title":"4 - Approve the CSR"},{"location":"troubleshooting/kubernetes/kubernetes/#5-retrieve-the-certificate-from-the-csr-object-its-base64-encoded","text":"kubectl get certificatesigningrequests demouser \\ -o jsonpath='{ .status.certificate }' | base64 --decode Let's go ahead and save the certificate into a local file. We're going to use this file to build a kubeconfig file to authenticate to the API Server with kubectl get certificatesigningrequests demouser -o jsonpath='{ .status.certificate }' | base64 --decode > calmuser.crt Check the contents of the file cat demouser.crt Read the certficate itself Key elements: Issuer is our CA, Validity one year, Subject CN=demousers openssl x509 -in demouser.crt -text -noout | head -n 15 Now that we have the certificate we can use that to build a kubeconfig file with to log into this cluster. We'll use demouser.key and demouser.crt ls demouser.*","title":"5 - Retrieve the certificate from the CSR object, it's base64 encoded"},{"location":"troubleshooting/kubernetes/kubernetes/#complete-alternative-a-create-a-self-signed-certificate","text":"The below command will generate a self-signed certificate valid for two years with sha256 \u2013days parameter to extend the validity. Ex: to have self-signed valid for two years. openssl req -x509 -sha256 -nodes -days 730 -newkey rsa:2048 -keyout demouser_self.key -out demouser_cert.pem","title":"COMPLETE ALTERNATIVE A - Create a Self-signed Certificate"},{"location":"troubleshooting/kubernetes/kubernetes/#misc-alternative-b","text":"","title":"MISC ALTERNATIVE B"},{"location":"troubleshooting/kubernetes/kubernetes/#create-a-csr-from-existing-certificate-and-private-key","text":"openssl x509 -x509toreq -in cert.pem -out example.csr -signkey example.key or just from existing private key openssl req \u2013out certificate.csr \u2013key existing.key \u2013new","title":"Create a CSR from existing certificate and private key"},{"location":"troubleshooting/kubernetes/kubernetes/#generate-a-csr-for-multi-domain-san-certificate-by-supplying-an-openssl-config-file","text":"openssl req -new -key example.key -out example.csr -config req.conf cat <<EOF | tee req.conf [req]prompt=nodefault_md = sha256distinguished_name = dnreq_extensions = req_ext [dn]CN=example.com [req_ext]subjectAltName=@alt_names [alt_names]DNS.1=example.comDNS.2=www.example.comDNS.3=ftp.example.com EOF","title":"Generate a CSR for multi-domain SAN certificate by supplying an openssl config file:"},{"location":"troubleshooting/kubernetes/kubernetes/#create-x509-certificates","text":"Create self-signed certificate and new private key from scratch: openssl req -nodes -newkey rsa:2048 -keyout example.key -out example.crt -x509 -days 365 Create a self signed certificate using existing CSR and private key: openssl x509 -req -in example.csr -signkey example.key -out example.crt -days 365","title":"Create X.509 certificates"},{"location":"troubleshooting/kubernetes/kubernetes/#troubleshooting-certificate-issues","text":"Verify that private key matches a certificate, CSR and Private Key openssl verify demouser.crt openssl rsa -in demouser.key \u2013check openssl rsa -noout -modulus -in demouser.key | openssl sha256 openssl x509 -noout -modulus -in demouser.crt | openssl sha256 openssl req -noout -modulus -in demouser.csr | openssl sha256 Verify a Certificate was Signed by a CA openssl verify -verbose -CAFile ca.crt domain.crt Verifty the Certificate Signer Authority openssl x509 -in certfile.pem -text \u2013noout openssl x509 -in demouser_cert.pem -noout -issuer -issuer_hash Check Hash Value of A Certificate openssl x509 -noout -hash -in demouser_cert.pem Verify certificate, when you have intermediate certificate chain . Root certificate is not a part of bundle , and should be configured as a trusted on your machine. openssl verify -untrusted demouser-intermediate.pem demouser.crt Verify certificate, when you have intermediate certificate chain and root certificate , that is not configured as a trusted one. openssl verify -CAFile root.crt -untrusted intermediate-ca-chain.pem child-demouser.crt Verify that certificate served by a remote server covers given host name. Useful to check your mutlidomain certificate properly covers all the host names. openssl s_client -verify_hostname www.example.com -connect example.com:443 TLS client to connect to a remote server Test SSL certificate of particular URL Connect to a server supporting TLS: openssl s_client -connect example.com:443 openssl s_client -host example.com -port 443 openssl s_client -connect yoururl.com:443 \u2013showcerts Connect to a server and show full certificate chain: openssl s_client -showcerts -host example.com -port 443 </dev/null Extract the certificate: openssl s_client -connect example.com:443 2>&1 < /dev/null | sed -n '/-----BEGIN/,/-----END/p' > certificate.pem Override SNI (Server Name Indication) extension with another server name. Useful for testing when multiple secure sites are hosted on same IP address: openssl s_client -servername www.example.com -host example.com -port 443 Measure SSL connection time without/with session reuse: openssl s_time -connect example.com:443 -new openssl s_time -connect example.com:443 -reuse Convert between encoding and container formats Convert certificate between DER and PEM formats: openssl x509 -in example.pem -outform der -out example.der openssl x509 -in example.der -inform der -out example.pem Check PEM File Certificate Expiration Date openssl x509 -noout -in certificate.pem -dates Check Certificate Expiration Date of SSL URL openssl s_client -connect google.com:443 2>/dev/null | openssl x509 -noout -enddate Check TLS Versions are accepted on URL openssl s_client -connect secureurl.com:443 \u2013tls1 openssl s_client -connect secureurl.com:443 \u2013tls1_1 openssl s_client -showcerts -servername rancher.10.38.20.81.nip.io -connect rancher.10.38.20.81.nip.io:443","title":"Troubleshooting Certificate Issues"},{"location":"troubleshooting/kubernetes/kubernetes/#capacity-planning","text":"Get vCPU Count from all nodes kubectl get nodes -o=jsonpath=\"{range .items[*]}{.metadata.name}{\\\"\\t\\\"} \\ {.status.capacity.cpu}{\\\"\\n\\\"}{end}\"","title":"Capacity Planning"},{"location":"troubleshooting/kubernetes/redhat/","text":"Fixing Image Pull Secret \u00b6 get current pull secret oc get secret pull-secret -n openshift-config --template='{{index .data \".dockerconfigjson\" | base64decode}}' https://access.redhat.com/solutions/4844461 You need to authenticate using a Bearer token, which you can get from the second section at https://cloud.redhat.com/openshift/token. OFFLINE_ACCESS_TOKEN=<> $ export BEARER=$(curl \\ --silent \\ --data-urlencode \"grant_type=refresh_token\" \\ --data-urlencode \"client_id=cloud-services\" \\ --data-urlencode \"refresh_token=${OFFLINE_ACCESS_TOKEN}\" \\ https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token | \\ jq -r .access_token) $ curl -X POST https://api.openshift.com/api/accounts_mgmt/v1/access_token --header \"Content-Type:application/json\" --header \"Authorization: Bearer $BEARER\" | jq { \"auths\": { \"cloud.openshift.com\": { \"auth\": \"<snip>\", \"email\": \"<user's email>\" }, \"quay.io\": { \"auth\": \"<snip>\", \"email\": \"<user's email>\" }, \"registry.connect.redhat.com\": { \"auth\": \"<snip>\", \"email\": \"<user's email>\" }, \"registry.redhat.io\": { \"auth\": \"<snip>\", \"email\": \"<user's email>\" } } } or directly | base64 | pbcopy and pbpaste into cmd line Changing the Global Pull Secret \u00b6 How to change the global pull secret in OCP 4 oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=.local/rh-pullsecret.json oc get secret pull-secret -n openshift-config --template='{{index .data \".dockerconfigjson\" | base64decode}}'","title":"Redhat"},{"location":"troubleshooting/kubernetes/redhat/#fixing-image-pull-secret","text":"get current pull secret oc get secret pull-secret -n openshift-config --template='{{index .data \".dockerconfigjson\" | base64decode}}' https://access.redhat.com/solutions/4844461 You need to authenticate using a Bearer token, which you can get from the second section at https://cloud.redhat.com/openshift/token. OFFLINE_ACCESS_TOKEN=<> $ export BEARER=$(curl \\ --silent \\ --data-urlencode \"grant_type=refresh_token\" \\ --data-urlencode \"client_id=cloud-services\" \\ --data-urlencode \"refresh_token=${OFFLINE_ACCESS_TOKEN}\" \\ https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token | \\ jq -r .access_token) $ curl -X POST https://api.openshift.com/api/accounts_mgmt/v1/access_token --header \"Content-Type:application/json\" --header \"Authorization: Bearer $BEARER\" | jq { \"auths\": { \"cloud.openshift.com\": { \"auth\": \"<snip>\", \"email\": \"<user's email>\" }, \"quay.io\": { \"auth\": \"<snip>\", \"email\": \"<user's email>\" }, \"registry.connect.redhat.com\": { \"auth\": \"<snip>\", \"email\": \"<user's email>\" }, \"registry.redhat.io\": { \"auth\": \"<snip>\", \"email\": \"<user's email>\" } } } or directly | base64 | pbcopy and pbpaste into cmd line","title":"Fixing Image Pull Secret"},{"location":"troubleshooting/kubernetes/redhat/#changing-the-global-pull-secret","text":"How to change the global pull secret in OCP 4 oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=.local/rh-pullsecret.json oc get secret pull-secret -n openshift-config --template='{{index .data \".dockerconfigjson\" | base64decode}}'","title":"Changing the Global Pull Secret"},{"location":"troubleshooting/kubernetes/references/","text":"Good References \u00b6 Troubleshooting \u00b6 Troubleshooting Kubernetes - https://learnk8s.io/a/a-visual-guide-on-troubleshooting-kubernetes-deployments/troubleshooting-kubernetes.v2.pdf Karbon Node Resource Recommendations - https://portal.nutanix.com/page/documents/details?targetId=Karbon-v2_2:kar-karbon-resource-usage-r.html","title":"Good References"},{"location":"troubleshooting/kubernetes/references/#good-references","text":"","title":"Good References"},{"location":"troubleshooting/kubernetes/references/#troubleshooting","text":"Troubleshooting Kubernetes - https://learnk8s.io/a/a-visual-guide-on-troubleshooting-kubernetes-deployments/troubleshooting-kubernetes.v2.pdf Karbon Node Resource Recommendations - https://portal.nutanix.com/page/documents/details?targetId=Karbon-v2_2:kar-karbon-resource-usage-r.html","title":"Troubleshooting"},{"location":"walkthroughs/nutanix/redhat-gitops/","text":"Exploring the OpenShift GitOps Operator \u00b6 https://developers.redhat.com/courses/gitops/getting-started-argocd-and-openshift-gitops-operator oc get pods -n openshift-console | grep console oc get routes console -n openshift-console oc get operators oc get routes -n openshift-gitops | grep openshift-gitops-server | awk '{print $2}' openshift-gitops-server-openshift-gitops.apps.ocp1.ntnxlab.local oc extract secret/openshift-gitops-cluster -n openshift-gitops --to=- ARGOCD_SERVER_URL=$(oc get routes -n openshift-gitops | grep openshift-gitops-server | awk '{print $2}') argocd login $ARGOCD_SERVER_URL argocd cluster list Deploying a Sample Application \u00b6 In this environment, we have some example manifesets taken from our sample GitOps repo . We'll be uisng this repo to test. These manifests include: A Namespace: bgd-ns.yaml A Deployment: bgd-deployment.yaml A Service: bgd-svc.yaml A Route: bgd-route.yaml Collectively, this is known as an Application within ArgoCD. Therefore, you must define it as such in order to apply these manifest in your cluster. Open up the Argo CD Application manifest: bgd-app.yaml Let's break this down a bit. ArgoCD's concept of a Project is different than OpenShift's. Here you're installing the application in ArgoCD's default project (.spec.project). NOT OpenShift's default project. The destination server is the server we installed ArgoCD on (noted as .spec.destination.server). The manifest repo where the YAML resides and the path to look for the YAML is under .spec.source. The .spec.syncPolicy is set to false. Note that you can have Argo CD automatically sync the repo. The last section .spec.sync just says what are you comparing the repo to. (Basically \"Compare the running config to the desired config\") The Application CR (CustomResource) can be applied by running the following: oc apply -f https://raw.githubusercontent.com/redhat-developer-demos/openshift-gitops-examples/main/components/applications/bgd-app.yaml oc get pods,svc,route -n bgd oc rollout status deploy/bgd -n bgd oc get route -n bgd oc -n bgd patch deploy/bgd --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/env/0/value\", \"value\":\"green\"}]' oc rollout status deploy/bgd -n bgd It's now out of sync in UI You can sync your app via the Argo CD by: First clicking SYNC Then clicking SYNCHRONIZE After the sync process is done, the Argo CD UI should mark the application as in sync. If you reload the page on the tab where the application is running. It should have returned to a blue square. argocd app sync bgd-app You can setup Argo CD to automatically correct drift by setting the Application manifest to do so. Here is an example snippet: spec: syncPolicy: automated: prune: true selfHeal: true","title":"Redhat gitops"},{"location":"walkthroughs/nutanix/redhat-gitops/#exploring-the-openshift-gitops-operator","text":"https://developers.redhat.com/courses/gitops/getting-started-argocd-and-openshift-gitops-operator oc get pods -n openshift-console | grep console oc get routes console -n openshift-console oc get operators oc get routes -n openshift-gitops | grep openshift-gitops-server | awk '{print $2}' openshift-gitops-server-openshift-gitops.apps.ocp1.ntnxlab.local oc extract secret/openshift-gitops-cluster -n openshift-gitops --to=- ARGOCD_SERVER_URL=$(oc get routes -n openshift-gitops | grep openshift-gitops-server | awk '{print $2}') argocd login $ARGOCD_SERVER_URL argocd cluster list","title":"Exploring the OpenShift GitOps Operator"},{"location":"walkthroughs/nutanix/redhat-gitops/#deploying-a-sample-application","text":"In this environment, we have some example manifesets taken from our sample GitOps repo . We'll be uisng this repo to test. These manifests include: A Namespace: bgd-ns.yaml A Deployment: bgd-deployment.yaml A Service: bgd-svc.yaml A Route: bgd-route.yaml Collectively, this is known as an Application within ArgoCD. Therefore, you must define it as such in order to apply these manifest in your cluster. Open up the Argo CD Application manifest: bgd-app.yaml Let's break this down a bit. ArgoCD's concept of a Project is different than OpenShift's. Here you're installing the application in ArgoCD's default project (.spec.project). NOT OpenShift's default project. The destination server is the server we installed ArgoCD on (noted as .spec.destination.server). The manifest repo where the YAML resides and the path to look for the YAML is under .spec.source. The .spec.syncPolicy is set to false. Note that you can have Argo CD automatically sync the repo. The last section .spec.sync just says what are you comparing the repo to. (Basically \"Compare the running config to the desired config\") The Application CR (CustomResource) can be applied by running the following: oc apply -f https://raw.githubusercontent.com/redhat-developer-demos/openshift-gitops-examples/main/components/applications/bgd-app.yaml oc get pods,svc,route -n bgd oc rollout status deploy/bgd -n bgd oc get route -n bgd oc -n bgd patch deploy/bgd --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/env/0/value\", \"value\":\"green\"}]' oc rollout status deploy/bgd -n bgd It's now out of sync in UI You can sync your app via the Argo CD by: First clicking SYNC Then clicking SYNCHRONIZE After the sync process is done, the Argo CD UI should mark the application as in sync. If you reload the page on the tab where the application is running. It should have returned to a blue square. argocd app sync bgd-app You can setup Argo CD to automatically correct drift by setting the Application manifest to do so. Here is an example snippet: spec: syncPolicy: automated: prune: true selfHeal: true","title":"Deploying a Sample Application"},{"location":"walkthroughs/nutanix/integrations/redhat-image-registry/","text":"https://github.com/nutanix/openshift/tree/main/operators/csi oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"managementState\":\"Removed\"}}' oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"managementState\":\"Managed\"}}' Exposing Image Registry \u00b6 oc patch configs.imageregistry.operator.openshift.io/cluster --patch '{\"spec\":{\"defaultRoute\":true}}' --type=merge Nutanix Volumes Image Registry \u00b6 export KUBECONFIG=~/openshift/auth/kubeconfig echo \"\"\"apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nutanix-volume provisioner: csi.nutanix.com parameters: csi.storage.k8s.io/provisioner-secret-name: ntnx-secret csi.storage.k8s.io/provisioner-secret-namespace: ntnx-system csi.storage.k8s.io/node-publish-secret-name: ntnx-secret csi.storage.k8s.io/node-publish-secret-namespace: ntnx-system csi.storage.k8s.io/controller-expand-secret-name: ntnx-secret csi.storage.k8s.io/controller-expand-secret-namespace: ntnx-system csi.storage.k8s.io/fstype: ext4 dataServiceEndPoint: 10.42.35.38:3260 storageContainer: Default storageType: NutanixVolumes #whitelistIPMode: ENABLED #chapAuth: ENABLED allowVolumeExpansion: true reclaimPolicy: Delete\"\"\" > nutanix-volumes-storageclass.yaml echo \"\"\"kind: PersistentVolumeClaim apiVersion: v1 metadata: name: image-registry-claim namespace: openshift-image-registry spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: nutanix-volume\"\"\" > nutanix-volumes-pvc.yaml Nutanix Dynamic Files Image Registry \u00b6 export KUBECONFIG=~/openshift/auth/kubeconfig echo \"\"\"kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: nutanix-files-dynamic provisioner: csi.nutanix.com parameters: dynamicProv: ENABLED nfsServerName: FedNFS #nfsServerName above is File Server Name in Prism without DNS suffix, not the FQDN. csi.storage.k8s.io/provisioner-secret-name: ntnx-secret csi.storage.k8s.io/provisioner-secret-namespace: ntnx-system storageType: NutanixFiles\"\"\" > nutanix-files-dynamic-storageclass.yaml echo \"\"\" kind: PersistentVolumeClaim apiVersion: v1 metadata: name: image-registry-claim namespace: openshift-image-registry spec: accessModes: - ReadWriteMany resources: requests: storage: 100Gi storageClassName: nutanix-files-dynamic\"\"\" > nutanix-files-dynamic-pvc.yaml oc apply -f nutanix-files-dynamic-storageclass.yaml oc apply -f nutanix-files-dynamic-pvc.yaml Patch OC Image Registry to use created PVC \u00b6 oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"managementState\":\"Managed\",\"storage\":{\"pvc\":{\"claim\":\"image-registry-claim\"}},\"rolloutStrategy\": \"Recreate\"}}' https://console-openshift-console.apps.ocp1.ntnxlab.local/k8s/cluster/imageregistry.operator.openshift.io~v1~Config/cluster/yaml Nutanix Objects s3 Image Registry \u00b6 https://FedS3.ntnxlab.local oc create secret generic image-registry-private-configuration-user --from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=B9lSXygeJT9D0jpRjbxu-eA91UZ1rrgQ --from-literal=REGISTRY_STORAGE_S3_SECRETKEY=jP4B_qHMZPo3vdJX-l4USABM-e7HAOFn --namespace openshift-image-registry Access Key: B9lSXygeJT9D0jpRjbxu-eA91UZ1rrgQ Secret Key: jP4B_qHMZPo3vdJX-l4USABM-e7HAOFn https://openshift-image-registry.FedS3.ntnxlab.local bucket: openshift-image-registry nx2Tech704! storage: s3: bucket: region: apiVersion: resources.cattle.io/v1 kind: Backup metadata: name: nightly-rancher-s3-backup spec: resourceSetName: rancher-resource-set retentionCount: 10 schedule: 0 0 * * * storageLocation: s3: bucketName: harvester-rke2-lab-bucket credentialSecretName: aws-s3-creds credentialSecretNamespace: default region: us-gov-east-1 endpoint: s3.us-gov-east-1.amazonaws.com https://podman.io/getting-started/installation sudo dnf -y install podman registry login and pull \u00b6 oc login -u kubeadmin -p oLVsN-6vhUA-ijrIW-VNhue https://api-int.apps.ocp1.ntnxlab.local:6443 podman login -u kubeadmin -p $(oc whoami -t) --tls-verify=false default-route-openshift-image-registry.apps.ocp1.ntnxlab.local podman pull docker.io/busybox podman tag docker.io/busybox default-route-openshift-image-registry.apps.ocp1.ntnxlab.local/openshi\u203aft/busybox podman images podman push --tls-verify=false default-route-openshift-image-registry.apps.ocp1.ntnxlab.local/openshift/busybox podman images podman rmi default-route-openshift-image-registry.apps.ocp1.ntnxlab.local/openshift/busybox podman images podman pull --tls-verify=false default-route-openshift-image-registry.apps.ocp1.ntnxlab.local/openshift/busybox podman images cleanup images \u00b6 podman rmi default-route-openshift-image-registry.apps.ocp1.ntnxlab.local/openshift/busybox docker.io/library/busybox kasten install \u00b6 SECRET=$(kubectl get sc -o=jsonpath='{.items[?(@.metadata.annotations.storageclass.kubernetes.io\\/is-default-class==\"true\")].parameters.csi.storage.k8s.io\\/provisioner-secret-name}') DRIVER=$(kubectl get sc -o=jsonpath='{.items[?(@.metadata.annotations.storageclass.kubernetes.io\\/is-default-class==\"true\")].provisioner}') cat <<EOF | kubectl apply -f - apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: default-snapshotclass driver: csi.nutanix.com parameters: storageType: NutanixVolumes csi.storage.k8s.io/snapshotter-secret-name: $SECRET csi.storage.k8s.io/snapshotter-secret-namespace: kube-system deletionPolicy: Delete EOF helm repo add kasten https://charts.kasten.io --force-update && helm repo update kubectl create ns kasten-io kubectl annotate volumesnapshotclass default-snapshotclass \\ k10.kasten.io/is-snapshot-class=true curl -s https://docs.kasten.io/tools/k10_primer.sh | bash https://10.42.35.37:9440/console/#login Prism UI Credentials: admin/nx2Tech704! CVM Credentials: nutanix/nx2Tech704!","title":"Redhat image registry"},{"location":"walkthroughs/nutanix/integrations/redhat-image-registry/#exposing-image-registry","text":"oc patch configs.imageregistry.operator.openshift.io/cluster --patch '{\"spec\":{\"defaultRoute\":true}}' --type=merge","title":"Exposing Image Registry"},{"location":"walkthroughs/nutanix/integrations/redhat-image-registry/#nutanix-volumes-image-registry","text":"export KUBECONFIG=~/openshift/auth/kubeconfig echo \"\"\"apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nutanix-volume provisioner: csi.nutanix.com parameters: csi.storage.k8s.io/provisioner-secret-name: ntnx-secret csi.storage.k8s.io/provisioner-secret-namespace: ntnx-system csi.storage.k8s.io/node-publish-secret-name: ntnx-secret csi.storage.k8s.io/node-publish-secret-namespace: ntnx-system csi.storage.k8s.io/controller-expand-secret-name: ntnx-secret csi.storage.k8s.io/controller-expand-secret-namespace: ntnx-system csi.storage.k8s.io/fstype: ext4 dataServiceEndPoint: 10.42.35.38:3260 storageContainer: Default storageType: NutanixVolumes #whitelistIPMode: ENABLED #chapAuth: ENABLED allowVolumeExpansion: true reclaimPolicy: Delete\"\"\" > nutanix-volumes-storageclass.yaml echo \"\"\"kind: PersistentVolumeClaim apiVersion: v1 metadata: name: image-registry-claim namespace: openshift-image-registry spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: nutanix-volume\"\"\" > nutanix-volumes-pvc.yaml","title":"Nutanix Volumes Image Registry"},{"location":"walkthroughs/nutanix/integrations/redhat-image-registry/#nutanix-dynamic-files-image-registry","text":"export KUBECONFIG=~/openshift/auth/kubeconfig echo \"\"\"kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: nutanix-files-dynamic provisioner: csi.nutanix.com parameters: dynamicProv: ENABLED nfsServerName: FedNFS #nfsServerName above is File Server Name in Prism without DNS suffix, not the FQDN. csi.storage.k8s.io/provisioner-secret-name: ntnx-secret csi.storage.k8s.io/provisioner-secret-namespace: ntnx-system storageType: NutanixFiles\"\"\" > nutanix-files-dynamic-storageclass.yaml echo \"\"\" kind: PersistentVolumeClaim apiVersion: v1 metadata: name: image-registry-claim namespace: openshift-image-registry spec: accessModes: - ReadWriteMany resources: requests: storage: 100Gi storageClassName: nutanix-files-dynamic\"\"\" > nutanix-files-dynamic-pvc.yaml oc apply -f nutanix-files-dynamic-storageclass.yaml oc apply -f nutanix-files-dynamic-pvc.yaml","title":"Nutanix Dynamic Files Image Registry"},{"location":"walkthroughs/nutanix/integrations/redhat-image-registry/#patch-oc-image-registry-to-use-created-pvc","text":"oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"managementState\":\"Managed\",\"storage\":{\"pvc\":{\"claim\":\"image-registry-claim\"}},\"rolloutStrategy\": \"Recreate\"}}' https://console-openshift-console.apps.ocp1.ntnxlab.local/k8s/cluster/imageregistry.operator.openshift.io~v1~Config/cluster/yaml","title":"Patch OC Image Registry to use created PVC"},{"location":"walkthroughs/nutanix/integrations/redhat-image-registry/#nutanix-objects-s3-image-registry","text":"https://FedS3.ntnxlab.local oc create secret generic image-registry-private-configuration-user --from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=B9lSXygeJT9D0jpRjbxu-eA91UZ1rrgQ --from-literal=REGISTRY_STORAGE_S3_SECRETKEY=jP4B_qHMZPo3vdJX-l4USABM-e7HAOFn --namespace openshift-image-registry Access Key: B9lSXygeJT9D0jpRjbxu-eA91UZ1rrgQ Secret Key: jP4B_qHMZPo3vdJX-l4USABM-e7HAOFn https://openshift-image-registry.FedS3.ntnxlab.local bucket: openshift-image-registry nx2Tech704! storage: s3: bucket: region: apiVersion: resources.cattle.io/v1 kind: Backup metadata: name: nightly-rancher-s3-backup spec: resourceSetName: rancher-resource-set retentionCount: 10 schedule: 0 0 * * * storageLocation: s3: bucketName: harvester-rke2-lab-bucket credentialSecretName: aws-s3-creds credentialSecretNamespace: default region: us-gov-east-1 endpoint: s3.us-gov-east-1.amazonaws.com https://podman.io/getting-started/installation sudo dnf -y install podman","title":"Nutanix Objects s3 Image Registry"},{"location":"walkthroughs/nutanix/integrations/redhat-image-registry/#registry-login-and-pull","text":"oc login -u kubeadmin -p oLVsN-6vhUA-ijrIW-VNhue https://api-int.apps.ocp1.ntnxlab.local:6443 podman login -u kubeadmin -p $(oc whoami -t) --tls-verify=false default-route-openshift-image-registry.apps.ocp1.ntnxlab.local podman pull docker.io/busybox podman tag docker.io/busybox default-route-openshift-image-registry.apps.ocp1.ntnxlab.local/openshi\u203aft/busybox podman images podman push --tls-verify=false default-route-openshift-image-registry.apps.ocp1.ntnxlab.local/openshift/busybox podman images podman rmi default-route-openshift-image-registry.apps.ocp1.ntnxlab.local/openshift/busybox podman images podman pull --tls-verify=false default-route-openshift-image-registry.apps.ocp1.ntnxlab.local/openshift/busybox podman images","title":"registry login and pull"},{"location":"walkthroughs/nutanix/integrations/redhat-image-registry/#cleanup-images","text":"podman rmi default-route-openshift-image-registry.apps.ocp1.ntnxlab.local/openshift/busybox docker.io/library/busybox","title":"cleanup images"},{"location":"walkthroughs/nutanix/integrations/redhat-image-registry/#kasten-install","text":"SECRET=$(kubectl get sc -o=jsonpath='{.items[?(@.metadata.annotations.storageclass.kubernetes.io\\/is-default-class==\"true\")].parameters.csi.storage.k8s.io\\/provisioner-secret-name}') DRIVER=$(kubectl get sc -o=jsonpath='{.items[?(@.metadata.annotations.storageclass.kubernetes.io\\/is-default-class==\"true\")].provisioner}') cat <<EOF | kubectl apply -f - apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: default-snapshotclass driver: csi.nutanix.com parameters: storageType: NutanixVolumes csi.storage.k8s.io/snapshotter-secret-name: $SECRET csi.storage.k8s.io/snapshotter-secret-namespace: kube-system deletionPolicy: Delete EOF helm repo add kasten https://charts.kasten.io --force-update && helm repo update kubectl create ns kasten-io kubectl annotate volumesnapshotclass default-snapshotclass \\ k10.kasten.io/is-snapshot-class=true curl -s https://docs.kasten.io/tools/k10_primer.sh | bash https://10.42.35.37:9440/console/#login Prism UI Credentials: admin/nx2Tech704! CVM Credentials: nutanix/nx2Tech704!","title":"kasten install"},{"location":"walkthroughs/nutanix/karbon/airgap/","text":"Deploying Airgap and Karbon Darksite \u00b6 Build web server \u00b6 Linux - https://portal.nutanix.com/page/documents/details?targetId=Life-Cycle-Manager-Dark-Site-Guide-v2_4:2-lcm-darksite-web-server-linux-t.html Windows - https://portal.nutanix.com/page/documents/details?targetId=Life-Cycle-Manager-Dark-Site-Guide-v2_4:2-lcm-darksite-web-server-windows-t.html Fetch Karbon Dark Site Bundle (if upgrading Karbon via LCM Darksite) \u00b6 https://portal.nutanix.com/page/documents/details?targetId=Life-Cycle-Manager-Dark-Site-Guide-v2_4:2-lcm-darksite-karbon-update-t.html Deploy Airgap \u00b6 https://portal.nutanix.com/page/documents/details?targetId=Karbon-v2_2:kar-karbon-airgap-c.html Create Directory on webserver \u00b6 sudo mkdir -p -m 755 /var/www/html/release/ntnx-2.2.0/ sudo curl https://download.nutanix.com/karbon/airgap/2.2.0/airgap-manifest.json -o airgap-manifest.json Execute commands via Karbonctl \u00b6 ./karbonctl login --pc-ip <> --pc-username admin --pc-password '<>' ./karbonctl airgap enable \\ --webserver-url http://10.38.18.176/release/ntnx-2.2.0/ \\ --vlan-name Primary --static-ip 10.38.18.150 \\ --storage-container SelfServiceContainer \\ --pe-cluster-name <> --pe-username admin \\ --pe-password '<>' \\ --dry-run","title":"airgap deployments"},{"location":"walkthroughs/nutanix/karbon/airgap/#deploying-airgap-and-karbon-darksite","text":"","title":"Deploying Airgap and Karbon Darksite"},{"location":"walkthroughs/nutanix/karbon/airgap/#build-web-server","text":"Linux - https://portal.nutanix.com/page/documents/details?targetId=Life-Cycle-Manager-Dark-Site-Guide-v2_4:2-lcm-darksite-web-server-linux-t.html Windows - https://portal.nutanix.com/page/documents/details?targetId=Life-Cycle-Manager-Dark-Site-Guide-v2_4:2-lcm-darksite-web-server-windows-t.html","title":"Build web server"},{"location":"walkthroughs/nutanix/karbon/airgap/#fetch-karbon-dark-site-bundle-if-upgrading-karbon-via-lcm-darksite","text":"https://portal.nutanix.com/page/documents/details?targetId=Life-Cycle-Manager-Dark-Site-Guide-v2_4:2-lcm-darksite-karbon-update-t.html","title":"Fetch Karbon Dark Site Bundle (if upgrading Karbon via LCM Darksite)"},{"location":"walkthroughs/nutanix/karbon/airgap/#deploy-airgap","text":"https://portal.nutanix.com/page/documents/details?targetId=Karbon-v2_2:kar-karbon-airgap-c.html","title":"Deploy Airgap"},{"location":"walkthroughs/nutanix/karbon/airgap/#create-directory-on-webserver","text":"sudo mkdir -p -m 755 /var/www/html/release/ntnx-2.2.0/ sudo curl https://download.nutanix.com/karbon/airgap/2.2.0/airgap-manifest.json -o airgap-manifest.json","title":"Create Directory on webserver"},{"location":"walkthroughs/nutanix/karbon/airgap/#execute-commands-via-karbonctl","text":"./karbonctl login --pc-ip <> --pc-username admin --pc-password '<>' ./karbonctl airgap enable \\ --webserver-url http://10.38.18.176/release/ntnx-2.2.0/ \\ --vlan-name Primary --static-ip 10.38.18.150 \\ --storage-container SelfServiceContainer \\ --pe-cluster-name <> --pe-username admin \\ --pe-password '<>' \\ --dry-run","title":"Execute commands via Karbonctl"}]}